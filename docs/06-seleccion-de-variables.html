<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 6 Métodos de selección de variables y regularización. | Notas Curso de Estadística II</title>
  <meta name="description" content="Capítulo 6 Métodos de selección de variables y regularización. | Notas Curso de Estadística II" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 6 Métodos de selección de variables y regularización. | Notas Curso de Estadística II" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 6 Métodos de selección de variables y regularización. | Notas Curso de Estadística II" />
  
  
  

<meta name="author" content="Maikol Solís Chacón y Luis Barboza Chinchilla" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="05-regresion-logistica.html"/>
<link rel="next" href="referencias.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Curso de Estadística</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html"><i class="fa fa-check"></i><b>2</b> Estimación no-paramétrica de densidades</a>
<ul>
<li class="chapter" data-level="2.1" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#histograma"><i class="fa fa-check"></i><b>2.1</b> Histograma</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#construcción-estadística"><i class="fa fa-check"></i><b>2.1.1</b> Construcción Estadística</a></li>
<li class="chapter" data-level="2.1.2" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#construcción-probabilística"><i class="fa fa-check"></i><b>2.1.2</b> Construcción probabilística</a></li>
<li class="chapter" data-level="2.1.3" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#propiedades-estadísticas"><i class="fa fa-check"></i><b>2.1.3</b> Propiedades estadísticas</a></li>
<li class="chapter" data-level="2.1.4" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#propiedades-estadísticas-1"><i class="fa fa-check"></i><b>2.1.4</b> Propiedades estadísticas</a></li>
<li class="chapter" data-level="2.1.5" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#sesgo"><i class="fa fa-check"></i><b>2.1.5</b> Sesgo</a></li>
<li class="chapter" data-level="2.1.6" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#varianza"><i class="fa fa-check"></i><b>2.1.6</b> Varianza</a></li>
<li class="chapter" data-level="2.1.7" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#error-cuadrático-medio"><i class="fa fa-check"></i><b>2.1.7</b> Error cuadrático medio</a></li>
<li class="chapter" data-level="2.1.8" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#error-cuadrático-medio-integrado"><i class="fa fa-check"></i><b>2.1.8</b> Error cuadrático medio integrado</a></li>
<li class="chapter" data-level="2.1.9" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#ancho-de-banda-óptimo-para-el-histograma"><i class="fa fa-check"></i><b>2.1.9</b> Ancho de banda óptimo para el histograma</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#estimación-de-densidades-basada-en-kernels."><i class="fa fa-check"></i><b>2.2</b> Estimación de densidades basada en kernels.</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#primera-construcción"><i class="fa fa-check"></i><b>2.2.1</b> Primera construcción</a></li>
<li class="chapter" data-level="2.2.2" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#otra-construcción"><i class="fa fa-check"></i><b>2.2.2</b> Otra construcción</a></li>
<li class="chapter" data-level="2.2.3" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#propiedades-estadísticas-2"><i class="fa fa-check"></i><b>2.2.3</b> Propiedades Estadísticas</a></li>
<li class="chapter" data-level="2.2.4" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#sesgo-1"><i class="fa fa-check"></i><b>2.2.4</b> Sesgo</a></li>
<li class="chapter" data-level="2.2.5" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#error-cuadrático-medio-y-error-cuadrático-medio-integrado"><i class="fa fa-check"></i><b>2.2.5</b> Error cuadrático medio y Error cuadrático medio integrado</a></li>
<li class="chapter" data-level="2.2.6" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#ancho-de-banda-óptimo"><i class="fa fa-check"></i><b>2.2.6</b> Ancho de banda óptimo</a></li>
<li class="chapter" data-level="2.2.7" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#intervalos-de-confianza-para-estimadores-de-densidad-no-paramétricos"><i class="fa fa-check"></i><b>2.2.7</b> Intervalos de confianza para estimadores de densidad no paramétricos</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#laboratorio"><i class="fa fa-check"></i><b>2.3</b> Laboratorio</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#efecto-de-distintos-kernels-en-la-estimación"><i class="fa fa-check"></i><b>2.3.1</b> Efecto de distintos Kernels en la estimación</a></li>
<li class="chapter" data-level="2.3.2" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#efecto-del-ancho-de-banda-en-la-estimación"><i class="fa fa-check"></i><b>2.3.2</b> Efecto del ancho de banda en la estimación</a></li>
<li class="chapter" data-level="2.3.3" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#ancho-de-banda-óptimo-1"><i class="fa fa-check"></i><b>2.3.3</b> Ancho de banda óptimo</a></li>
<li class="chapter" data-level="2.3.4" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#validación-cruzada-1"><i class="fa fa-check"></i><b>2.3.4</b> Validación cruzada</a></li>
<li class="chapter" data-level="2.3.5" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#temas-adicionales"><i class="fa fa-check"></i><b>2.3.5</b> Temas adicionales</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="01-estimacion-densidades-no-parametricas.html"><a href="01-estimacion-densidades-no-parametricas.html#ejercicios"><i class="fa fa-check"></i><b>2.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="02-jacknife-bootstrap.html"><a href="02-jacknife-bootstrap.html"><i class="fa fa-check"></i><b>3</b> Jackknife y Bootstrap</a>
<ul>
<li class="chapter" data-level="3.1" data-path="02-jacknife-bootstrap.html"><a href="02-jacknife-bootstrap.html#caso-concreto"><i class="fa fa-check"></i><b>3.1</b> Caso concreto</a></li>
<li class="chapter" data-level="3.2" data-path="02-jacknife-bootstrap.html"><a href="02-jacknife-bootstrap.html#jackknife"><i class="fa fa-check"></i><b>3.2</b> Jackknife</a></li>
<li class="chapter" data-level="3.3" data-path="02-jacknife-bootstrap.html"><a href="02-jacknife-bootstrap.html#bootstrap"><i class="fa fa-check"></i><b>3.3</b> Bootstrap</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="02-jacknife-bootstrap.html"><a href="02-jacknife-bootstrap.html#intervalos-de-confianza"><i class="fa fa-check"></i><b>3.3.1</b> Intervalos de confianza</a></li>
<li class="chapter" data-level="3.3.2" data-path="02-jacknife-bootstrap.html"><a href="02-jacknife-bootstrap.html#resumiendo"><i class="fa fa-check"></i><b>3.3.2</b> Resumiendo</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="02-jacknife-bootstrap.html"><a href="02-jacknife-bootstrap.html#ejercicios-1"><i class="fa fa-check"></i><b>3.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html"><i class="fa fa-check"></i><b>4</b> Métodos lineales de regresión</a>
<ul>
<li class="chapter" data-level="4.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#introducción-al-aprendizaje-estadístico."><i class="fa fa-check"></i><b>4.1</b> Introducción al Aprendizaje Estadístico.</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#formas-de-estimar-f"><i class="fa fa-check"></i><b>4.1.1</b> Formas de estimar <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="4.1.2" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#medidas-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>4.1.2</b> Medidas de bondad de ajuste</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#regresión-lineal"><i class="fa fa-check"></i><b>4.2</b> Regresión lineal</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#forma-matricial"><i class="fa fa-check"></i><b>4.2.1</b> Forma matricial</a></li>
<li class="chapter" data-level="4.2.2" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#laboratorio-1"><i class="fa fa-check"></i><b>4.2.2</b> Laboratorio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#propiedades-estadísticas-3"><i class="fa fa-check"></i><b>4.3</b> Propiedades estadísticas</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#prueba-t"><i class="fa fa-check"></i><b>4.3.1</b> Prueba <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="4.3.2" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#prueba-f"><i class="fa fa-check"></i><b>4.3.2</b> Prueba <span class="math inline">\(F\)</span></a></li>
<li class="chapter" data-level="4.3.3" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#laboratorio-2"><i class="fa fa-check"></i><b>4.3.3</b> Laboratorio</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#medida-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>4.4</b> Medida de bondad de ajuste</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#laboratorio-3"><i class="fa fa-check"></i><b>4.4.1</b> Laboratorio</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#predicción"><i class="fa fa-check"></i><b>4.5</b> Predicción</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#laboratorio-4"><i class="fa fa-check"></i><b>4.5.1</b> Laboratorio</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#interacciones"><i class="fa fa-check"></i><b>4.6</b> Interacciones</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#laboratorio-5"><i class="fa fa-check"></i><b>4.6.1</b> Laboratorio</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#supuestos"><i class="fa fa-check"></i><b>4.7</b> Supuestos</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#chequeos-básicos-de-las-hipótesis-de-regresión-lineal"><i class="fa fa-check"></i><b>4.7.1</b> Chequeos básicos de las hipótesis de regresión lineal</a></li>
<li class="chapter" data-level="4.7.2" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#otros-chequeos-importantes"><i class="fa fa-check"></i><b>4.7.2</b> Otros chequeos importantes</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="04-metodos-lineares-regresion.html"><a href="04-metodos-lineares-regresion.html#ejercicios-2"><i class="fa fa-check"></i><b>4.8</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html"><i class="fa fa-check"></i><b>5</b> Regresión Logística</a>
<ul>
<li class="chapter" data-level="5.1" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#preliminares"><i class="fa fa-check"></i><b>5.1</b> Preliminares</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#oportunidad-relativa-odds-ratio"><i class="fa fa-check"></i><b>5.1.1</b> Oportunidad relativa (Odds Ratio)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#máxima-verosimilitud"><i class="fa fa-check"></i><b>5.2</b> Máxima verosimilitud</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#resultados-adicionales"><i class="fa fa-check"></i><b>5.2.1</b> Resultados adicionales</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#diágnosticos-del-modelo"><i class="fa fa-check"></i><b>5.3</b> Diágnosticos del modelo</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#supuesto-de-linealidad"><i class="fa fa-check"></i><b>5.3.1</b> Supuesto de linealidad</a></li>
<li class="chapter" data-level="5.3.2" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#valores-de-gran-influencia"><i class="fa fa-check"></i><b>5.3.2</b> Valores de gran influencia</a></li>
<li class="chapter" data-level="5.3.3" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#multicolinealidad-1"><i class="fa fa-check"></i><b>5.3.3</b> Multicolinealidad</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#predicción-y-poder-de-clasificación"><i class="fa fa-check"></i><b>5.4</b> Predicción y poder de clasificación</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#curva-roc"><i class="fa fa-check"></i><b>5.4.1</b> Curva ROC</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="05-regresion-logistica.html"><a href="05-regresion-logistica.html#ejercicios-3"><i class="fa fa-check"></i><b>5.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html"><i class="fa fa-check"></i><b>6</b> Métodos de selección de variables y regularización.</a>
<ul>
<li class="chapter" data-level="6.1" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#estimación-del-error-de-prueba"><i class="fa fa-check"></i><b>6.1</b> Estimación del error de prueba</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#técnica-de-conjunto-de-validación"><i class="fa fa-check"></i><b>6.1.1</b> Técnica de conjunto de validación</a></li>
<li class="chapter" data-level="6.1.2" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#validación-cruzada-leave-one-out-loocv"><i class="fa fa-check"></i><b>6.1.2</b> Validación cruzada “Leave-One-Out” (LOOCV)</a></li>
<li class="chapter" data-level="6.1.3" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#validación-cruzada-k-veces"><i class="fa fa-check"></i><b>6.1.3</b> Validación cruzada <span class="math inline">\(k-\)</span>veces</a></li>
<li class="chapter" data-level="6.1.4" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#validación-cruzada-para-clasificación"><i class="fa fa-check"></i><b>6.1.4</b> Validación cruzada para clasificación</a></li>
<li class="chapter" data-level="6.1.5" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#otras-medidas-de-error-de-prueba"><i class="fa fa-check"></i><b>6.1.5</b> Otras medidas de error de prueba</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#selección-de-variables"><i class="fa fa-check"></i><b>6.2</b> Selección de variables</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#selección-del-mejor-subconjunto."><i class="fa fa-check"></i><b>6.2.1</b> Selección del mejor subconjunto.</a></li>
<li class="chapter" data-level="6.2.2" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#selección-de-modelos-hacia-adelante-forward-stepwise-selection"><i class="fa fa-check"></i><b>6.2.2</b> Selección de modelos hacia adelante (<strong>Forward Stepwise Selection</strong>)</a></li>
<li class="chapter" data-level="6.2.3" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#selección-de-modelos-hacia-atrás-backward-stepwise-selection"><i class="fa fa-check"></i><b>6.2.3</b> Selección de modelos hacia atrás (<strong>Backward Stepwise Selection</strong>)</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#métodos-de-regularización"><i class="fa fa-check"></i><b>6.3</b> Métodos de regularización</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#regresión-ridge"><i class="fa fa-check"></i><b>6.3.1</b> Regresión Ridge</a></li>
<li class="chapter" data-level="6.3.2" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#regresión-lasso"><i class="fa fa-check"></i><b>6.3.2</b> Regresión Lasso</a></li>
<li class="chapter" data-level="6.3.3" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#explicación-gráfica"><i class="fa fa-check"></i><b>6.3.3</b> Explicación gráfica</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#laboratorio-6"><i class="fa fa-check"></i><b>6.4</b> Laboratorio</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#cross-validation"><i class="fa fa-check"></i><b>6.4.1</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.4.2" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#regresión-lasso-1"><i class="fa fa-check"></i><b>6.4.2</b> Regresión Lasso</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="06-seleccion-de-variables.html"><a href="06-seleccion-de-variables.html#ejercicios-4"><i class="fa fa-check"></i><b>6.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notas Curso de Estadística II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="06-seleccion-de-variables.html#cb305-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">tinytex.verbose =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
</div>
</div>
<div id="métodos-de-selección-de-variables-y-regularización." class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Capítulo 6</span> Métodos de selección de variables y regularización.<a href="06-seleccion-de-variables.html#métodos-de-selección-de-variables-y-regularización." class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="estimación-del-error-de-prueba" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Estimación del error de prueba<a href="06-seleccion-de-variables.html#estimación-del-error-de-prueba" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recuerden que el error de prueba es el error promedio que se obtiene al usar un método de aprendizaje estadístico para predecir una observación que no está en el conjunto de calibración o entrenamiento. Si se tuviera un conjunto de prueba designado para medir el error de prueba, este último sería muy fácil de calcular. Cuando no se tiene un conjunto de prueba designado, entonces hay dos formas de estimar el error de prueba:</p>
<ol style="list-style-type: lower-alpha">
<li>Estimar indirectamente el error de prueba al hacerle un ajuste al error de entrenamiento. Es decir, agregarle artificialmente la variabilidad no observada, con el fin de corregir el exceso de sesgo producto de sobreajuste.</li>
<li>Estimar el error de prueba usando técnicas de muestreo sobre el conjunto de entrenamiento.</li>
</ol>
<p>Por ahora explicaremos la segunda solución:</p>
<div id="técnica-de-conjunto-de-validación" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Técnica de conjunto de validación<a href="06-seleccion-de-variables.html#técnica-de-conjunto-de-validación" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dividir aleatoriamente los datos totales en 2 partes:</p>
<ul>
<li>Conjunto de entrenamiento: en donde se ajusta el modelo.</li>
<li>Conjunto de validación o prueba: en donde el modelo ajustado se usa para realizar predicciones.</li>
</ul>
<p>Se usa una medida de validación en cada uno de los dos conjuntos (<span class="math inline">\(MSE\)</span> en el caso cuantitativo)</p>
<ol style="list-style-type: decimal">
<li>Puede que el estimador del error de prueba tenga una varianza muy alta, dependiendo de las observaciones que se incluyeron en la muestra de prueba.</li>
<li>El error de validación tiende a ser mayor que el error de entrenamiento debido a que se usa un número pequeño de observaciones en el conjunto de entrenamiento. (sobreestimación del error de prueba o validación)</li>
</ol>
</div>
<div id="validación-cruzada-leave-one-out-loocv" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Validación cruzada “Leave-One-Out” (LOOCV)<a href="06-seleccion-de-variables.html#validación-cruzada-leave-one-out-loocv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una sola observación <span class="math inline">\(\left( X_{i}, Y_{i} \right)\)</span> se usa en el conjunto de validación. Observaciones restantes, se usan en el conjunto de entrenamiento. Este proceso se repite para <span class="math inline">\(i=1,\ldots,n\)</span>.</p>
<p>Defina</p>
<p><span class="math display">\[\begin{equation*}
MSE_{i} =(Y_{i}-\hat{Y}_{i})^{2}
\end{equation*}\]</span></p>
<p>como el error cometido por usar la observación <span class="math inline">\(i\)</span> como muestra de prueba y el resto de valores como muestra de entrenamiento.</p>
<p>El estimador LOOCV es</p>
<p><span class="math display">\[\begin{equation*}
CV_{n} = \frac{1}{n} \sum_{i=1}^{n} MSE_{i}
\end{equation*}\]</span></p>
<p><strong>Ventajas</strong></p>
<ol style="list-style-type: decimal">
<li>Menos sesgo. (Conjunto de prueba de tamaño casi igual que los datos totales). Es decir el error de prueba no tiende a sobrestimarse.</li>
<li>No hay aleatoriedad en la aproximación del error de prueba producto de la separación conjunto de prueba-entrenamiento.</li>
</ol>
<p><strong>Desventaja</strong>: Puede ser lento, dependiendo de la cantidad de datos.</p>
<p>En el caso de un modelo lineal, se puede calcular <span class="math inline">\(CV_n\)</span> es una sola iteración:</p>
<p><span class="math display">\[\begin{align*}
CV_n=\frac 1 n \sum_{i=1}^n\left(\frac{y_i-\hat y_i}{1-h_{ii}}\right)^2
\end{align*}\]</span></p>
</div>
<div id="validación-cruzada-k-veces" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Validación cruzada <span class="math inline">\(k-\)</span>veces<a href="06-seleccion-de-variables.html#validación-cruzada-k-veces" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Se aplica el mismo principio que LOOCV, pero se divide la muestra en <span class="math inline">\(k\)</span> distintas partes (folds). El estimador del error de prueba es:</p>
<p><span class="math display">\[\begin{equation*}
CV_{k} = \frac{1}{k} \sum_{i=1}^{k} MSE_{i}
\end{equation*}\]</span></p>
<p>donde en este caso <span class="math inline">\(MSE_i\)</span> es el MSE obtenido al utilizar la <span class="math inline">\(i\)</span>-ésima parte de la muestra como conjunto de prueba y las restantes <span class="math inline">\(k-1\)</span> partes como conjunto de entrenamiento. Los valores usuales de <span class="math inline">\(k\)</span> son 5 o 10. Note que LOOCV es un caso especial de esta técnica de validación tomando <span class="math inline">\(k=n\)</span>.</p>
<p><strong>Ventaja</strong>: Es más económico</p>
<p><strong>Desventaja</strong>: Nivel intermedio de sesgo respecto a las dos técnicas anteriores.</p>
<ul>
<li>Recuerden que entre más grande sea el conjunto de entrenamiento, menos sesgo hay en la estimación de error de prueba.</li>
<li>El nivel de correlación entre los MSE’s de los conjuntos de prueba aumenta si los tamaños de entrenamiento aumentan. Por lo tanto la varianza del error de prueba estimado es mayor en estos casos.</li>
</ul>
<p><span class="math display">\[\begin{equation*}
\frac{n}{2} &lt; \frac{(k-1)n}{k} &lt; n-1
\end{equation*}\]</span></p>
</div>
<div id="validación-cruzada-para-clasificación" class="section level3 hasAnchor" number="6.1.4">
<h3><span class="header-section-number">6.1.4</span> Validación cruzada para clasificación<a href="06-seleccion-de-variables.html#validación-cruzada-para-clasificación" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Se usa</p>
<p><span class="math display">\[\begin{equation*}
CV_{n} = \frac{1}{n} \sum_{i=1}^{n} Err_{i}
\end{equation*}\]</span></p>
<p>donde <span class="math inline">\(Err_i = I_{Y_i \neq \hat{Y}_{i}}\)</span> y <span class="math inline">\(CV_{k}\)</span> se define de manera análoga.</p>
</div>
<div id="otras-medidas-de-error-de-prueba" class="section level3 hasAnchor" number="6.1.5">
<h3><span class="header-section-number">6.1.5</span> Otras medidas de error de prueba<a href="06-seleccion-de-variables.html#otras-medidas-de-error-de-prueba" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="r2-ajustado-1" class="section level4 hasAnchor" number="6.1.5.1">
<h4><span class="header-section-number">6.1.5.1</span> <span class="math inline">\(R^2\)</span> ajustado<a href="06-seleccion-de-variables.html#r2-ajustado-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recuerde que <span class="math inline">\(R^2 = 1 - \dfrac{RSS}{TSS}\)</span>. Como <span class="math inline">\(RSS\)</span> decrece si se le agrega más variables, entonces <span class="math inline">\(R^2 \nearrow 1\)</span>. Recuerden que <span class="math inline">\(RSS = \sum(y_i-\hat{y}_i)^2\)</span> y <span class="math inline">\(TSS = \sum(y_i-\bar{y}_i)^2\)</span>. En este caso definimos:</p>
<p><span class="math display">\[R^2 \text{ ajustado}= 1-\dfrac{\dfrac{RSS}{n-p-1}}{\dfrac{TSS}{n-1}}\]</span></p>
</div>
<div id="c_p-de-mallows" class="section level4 hasAnchor" number="6.1.5.2">
<h4><span class="header-section-number">6.1.5.2</span> <span class="math inline">\(C_p\)</span> de Mallows<a href="06-seleccion-de-variables.html#c_p-de-mallows" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Este estadístico se define como</p>
<p><span class="math display">\[ C_p = \dfrac{RSS}{\hat\sigma^2} + 2p-n \]</span></p>
<p>donde <span class="math inline">\(p\)</span> es el número de predictores y <span class="math inline">\(\hat\sigma^2\)</span> es el estimador de la varianza de los errores <span class="math inline">\(\epsilon\)</span>. Si <span class="math inline">\(\hat\sigma^2\)</span> es insesgado de <span class="math inline">\(\sigma^2\)</span>, entonces <span class="math inline">\(C_p\)</span> es un estimador insesgado del <span class="math inline">\(MSE\)</span> de prueba.</p>
<p>Para regresión ordinaria sabemos que <span class="math inline">\(\hat{\boldsymbol{\beta}}_{p}=\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime} \mathbf{Y}\)</span>. Nuestro caso ideal sería hacer el MSE lo más pequeño posible entre todos los posibles modleos,
<span class="math display">\[\begin{equation*}
\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}.
\end{equation*}\]</span></p>
<p>Con esto en mente, calculemos el RSS del modelo,</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\operatorname{RSS}(p) &amp;=\sum_{n=1}^{N}\left(y_{n}-\mathbf{x}_{n} \hat{\boldsymbol{\beta}}_{p}\right)^{2} \\
&amp;=\left(\mathbf{Y}-\mathbf{X}_{p} \hat{\boldsymbol{\beta}}_{p}\right)^{\prime}\left(\mathbf{Y}-\mathbf{X}_{p} \hat{\boldsymbol{\beta}}_{p}\right) \\
&amp;=\mathbf{Y}^{\prime}\left(\mathbf{I}_{N}-\mathbf{X}_{p}\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime}\right) \mathbf{Y}
\end{aligned}
\end{equation*}\]</span></p>
<p>Usando este resultado para matrices</p>
<p><span class="math display">\[\begin{equation*}
\mathrm{E}\left[\mathbf{Y}^{\prime} \mathbf{A Y}\right]=\mathrm{E}\left[\mathbf{Y}^{\prime}\right] \mathbf{A E}[\mathbf{Y}]+\operatorname{tr}[\mathbf{\Sigma} \mathbf{A}]
\end{equation*}\]</span></p>
<p>y donde <span class="math inline">\(\boldsymbol{\Sigma}\)</span> es la matriz de covarianza de <span class="math inline">\(\mathbf{Y}\)</span>, encontramos que</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\mathrm{E}[\operatorname{RSS}(p)] &amp;=\mathrm{E}\left[\mathbf{Y}^{\prime}\left(\mathbf{I}_{N}-\mathbf{X}_{p}\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime}\right) \mathbf{Y}\right] \\
&amp;=\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}+\operatorname{tr}\left[\mathbf{I}_{N}-\mathbf{X}_{p}\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime}\right] \sigma^{2} \\
&amp;=\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}+\sigma^{2}\left(N-\operatorname{tr}\left[\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1}\right]\right) \\
&amp;=\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}+\sigma^{2}(N-p)
\end{aligned}
\end{equation*}\]</span></p>
<p>Note que si el modelo verddero tiene <span class="math inline">\(p\)</span> parametros, entocnes <span class="math inline">\(\mathrm{E}\left[\mathrm{C}_{p}\right]=p\)</span>. Esto muestra por qué, si un modelo es correcto, <span class="math inline">\(\mathrm{C}_{p}\)</span> tenderá a estar cerca de <span class="math inline">\(p\)</span></p>
<p>Un problema con el criterio <span class="math inline">\(\mathrm{C}_{p}\)</span> es que tenemos que encontrar una estimación apropiada de <span class="math inline">\(\sigma^{2}\)</span> para usar con todos los valores de <span class="math inline">\(p\)</span>.</p>
</div>
<div id="estimador-de-máxima-verosimilitud-mle" class="section level4 hasAnchor" number="6.1.5.3">
<h4><span class="header-section-number">6.1.5.3</span> Estimador de máxima verosimilitud (MLE)<a href="06-seleccion-de-variables.html#estimador-de-máxima-verosimilitud-mle" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Supongamos para el conjunto de datos disponible una medida de ajuste del modelo <span class="math inline">\(-\ln L(\hat{\beta} | x)=\ell(\beta)\)</span>. Así si el valor es grande, entonces los parámetros <span class="math inline">\(\beta\)</span>s serían los correctos. Si se define <span class="math inline">\(\overline{\ell}(\beta)=\mathbb{E}(\ell(\beta\mid X))\)</span> como la log verosimilitud poblacional.</p>
<p>El problema con está medida es que no penaliza el ingreso de nuevas variables al modelo.</p>
</div>
<div id="akaike-information-criterion-aic." class="section level4 hasAnchor" number="6.1.5.4">
<h4><span class="header-section-number">6.1.5.4</span> Akaike Information Criterion (AIC).<a href="06-seleccion-de-variables.html#akaike-information-criterion-aic." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El AIC se define de la siguiente manera:</p>
<p><span class="math display">\[
AIC = -2\ell(\hat{\beta}) + 2p
\]</span></p>
<p><em>Explicación breve</em>: La idea del AIC es ajustar el riesgo empírico <span class="math inline">\(\hat{R}_n = -\ell(\hat{\beta})\)</span> con respecto al riesgo verdadero <span class="math inline">\(R(\hat{\beta}) = \mathbb{E}(-n\overline{\ell}(\hat{\beta}))\)</span>. Se puede probar que asintóticamente</p>
<p><span class="math display">\[\begin{equation*}
\mathbb{E}\left(\widehat{R}_{n}\left(\hat{\beta}_{n}\right)\right)-R\left(\widehat{\beta}_{n}\right)=-n \mathbb{E}\left(\left(\widehat{\beta}_{n}-\beta^{*}\right)^{T} I\left(\beta^{*}\right)\left(\widehat{\beta}_{n}-\beta^{*}\right)\right)
\end{equation*}\]</span></p>
<p>Por el curso de estadísitica I se puede probar que
<span class="math display">\[\begin{equation*}
\sqrt{n}\left(\hat{\beta}_{n}-\beta^{*}\right) \approx N\left(0, I^{-1}\left(\beta^{*}\right)\right)
\end{equation*}\]</span></p>
<p>Por lo tanto,</p>
<p><span class="math display">\[\begin{equation*}
n\left(\widehat{\beta}_{n}-\beta^{*}\right)^{T} I\left(\beta^{*}\right)\left(\widehat{\beta}_{n}-\beta^{*}\right) \approx \chi_{d}^{2}
\end{equation*}\]</span></p>
<p>lo cual implica que
<span class="math display">\[\begin{equation*}
n \mathbb{E}\left(\left(\widehat{\beta}_{n}-\beta^{*}\right)^{T} I\left(\beta^{*}\right)\left(\widehat{\beta}_{n}-\beta^{*}\right)\right)=d
\end{equation*}\]</span></p>
<p>Entonces para asegurarnos de que el estimador sea insesgado asintóticamente, debemos redefinir nuestro riesgo empírico timador sumándole un término <span class="math inline">\(p\)</span></p>
<p><span class="math display">\[\widehat{R}_{n}\left(\widehat{\beta}_{n}\right)+d=-\ell_{n}+d\]</span></p>
</div>
<div id="bayesian-information-criterion-bic" class="section level4 hasAnchor" number="6.1.5.5">
<h4><span class="header-section-number">6.1.5.5</span> Bayesian Information Criterion (BIC)<a href="06-seleccion-de-variables.html#bayesian-information-criterion-bic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Este criterior se parece al AIC pero modificado con un <span class="math inline">\(\log(n)\)</span>,</p>
<p><span class="math display">\[
BIC = -2\ell(\hat{\beta}) + \log(n)p.
\]</span></p>
<p>Para cada modelo <span class="math inline">\(m\)</span> Escriba la regla de Bayes definida en el curso anterior</p>
<p><span class="math display">\[\begin{equation*}
\pi\left(m \mid X_{1}, \cdots, X_{n}\right)=\frac{\pi\left(m, X_{1}, \cdots, X_{n}\right)}{L\left(X_{1}, \cdots, X_{n}\right)} \propto L\left(X_{1}, \cdots, X_{n} \mid m\right) \pi(m)
\end{equation*}\]</span></p>
<p>Se puede demostrar que</p>
<p><span class="math display">\[\begin{equation*}
L\left(X_{1}, \cdots, X_{n} \mid \theta, m\right) \approx e^{\ell_{n}-n\left(\theta-\theta^{*}\right)^{T} I\left(\theta^{*}\right)\left(\theta-\theta^{*}\right)}
\end{equation*}\]</span></p>
<p>Asumiento que la variable respuesta <span class="math inline">\(Y\)</span> es gaussiana, se puede simplificar la log-verosimilitud como</p>
<p><span class="math display">\[\begin{equation*}
\log p\left(X_{1}, \cdots, X_{n} \mid m\right) \approx \ell_{n}-\frac{d}{2} \log n+\frac{d}{2} \log (2 \pi)+\log \operatorname{det}\left(I\left(\theta^{*}\right)\right)+\log \pi\left(\hat{\theta}_{n} \mid m\right)
\end{equation*}\]</span></p>
<p>Las únicas dos cantidades que incrementan con <span class="math inline">\(n\)</span> son las dos primeras. Se multiplica por -2 para tener el BIC.</p>
<p>En el caso de datos gaussiano se tendría lo siguiente:</p>
<p><span class="math display">\[\begin{align*}
BIC = \frac{1}{n}(RSS+2\log (n)p\hat \sigma^2).
\end{align*}\]</span></p>
</div>
<div id="notas-adicionales" class="section level4 hasAnchor" number="6.1.5.6">
<h4><span class="header-section-number">6.1.5.6</span> Notas adicionales<a href="06-seleccion-de-variables.html#notas-adicionales" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Una explicación detallada de cada medida la pueden encontrar en el Capítulo 7 <span class="citation">(<strong>Hastie2009a?</strong>)</span> o en el artículo <span class="citation">(Cavanaugh and Neath 2019)</span>.</li>
<li>La validación cruzada LOOCV es asintóticamente equivalente al AIC para modelos de regresión lineal múltiple <span class="citation">(Stone 1977)</span>.</li>
<li>El AIC ajusta el modo que el riesgo o verosimilitud empírica o real sean insesgadas. Es decir, bajo la observación de nuevos datos, el error que se cometería debería ser cercano a 0.</li>
<li>Aunque el BIC se parece al AIC, el razonamiento es algo diferente. En la construcción de BIC estamos seleccionando el modelo con mayor evidencia según los datos. Cuando los datos se generan de hecho a partir de uno de los modelos en la colección de modelos que estamos eligiendo, el posterior se concentrará en este modelo correcto.<br />
</li>
<li>Una forma de interpretar ambos criterios es que AIC elige el mejor modelo predictivo, mientras que BIC intenta seleccionar el modelo verdadero si existe en el conjunto de modelos.</li>
</ul>
</div>
</div>
</div>
<div id="selección-de-variables" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Selección de variables<a href="06-seleccion-de-variables.html#selección-de-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cuando se construye un modelo de regresión (lineal o logística) existe la posibilidad de que existan más variables que datos disponibles. En este caso definitivamente la matriz de diseño <span class="math inline">\(X\)</span> no sería de rango completo, y otros estadísticos no tendrían una definición clara, por ejemplo el <span class="math inline">\(R^{2}\)</span> ajustado tenía un factor <span class="math inline">\(n-p-1\)</span> en el denominador y si <span class="math inline">\(n&gt;p\)</span> este tipo de indicador no se podría estimar.</p>
<p>En este capítulo veremos cómo construir modelos más simples y cómo hacer comparaciones entre ellos.</p>
<div id="selección-del-mejor-subconjunto." class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Selección del mejor subconjunto.<a href="06-seleccion-de-variables.html#selección-del-mejor-subconjunto." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En este caso trataremos de seleccionar el mejor subconjunto de un total de <span class="math inline">\(p\)</span> variables. Claramente si escogieramos solo <span class="math inline">\(k\)</span> variables existiría un total de <span class="math inline">\(\binom{p}{k}\)</span> modelos diferentes que escoger. Por lo tanto existe un total de <span class="math inline">\(2^p\)</span> posibles modelos para escoger con cualquier número de covariables.</p>
<p>El algoritmo para este caso sería:</p>
<p><em>Algoritmo:</em></p>
<ol style="list-style-type: decimal">
<li><p>Sea <span class="math inline">\(M_0\)</span> el modelo nulo.</p></li>
<li><p>Para <span class="math inline">\(k=1,2,\dots,p\)</span> (número de variables),</p>
<ol style="list-style-type: lower-alpha">
<li>Ajuste todos los <span class="math inline">\(\binom{p}{k}\)</span> modelos que contengan <span class="math inline">\(k\)</span> predictores.</li>
<li>Seleccione el mejor entre esos <span class="math inline">\(\binom{p}{k}\)</span> modelos. El “mejor” es el que tenga el <span class="math inline">\(RSS\)</span> menor, o el <span class="math inline">\(R^2\)</span> más grande. Llame a este modelo <span class="math inline">\(M_k\)</span>.</li>
</ol></li>
<li><p>Seleccione el mejor modelo entre <span class="math inline">\(M_0,M_1,\dots,M_p\)</span> usando el error de validación cruzada, <span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span> o <span class="math inline">\(R^2\)</span> ajustado.</p></li>
</ol>
<p><strong>Nota: Más adelante veremos qué es validación cruzada, <span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span> y <span class="math inline">\(BIC\)</span></strong></p>
<p><em>Ejemplo:</em> <span class="math inline">\(Y = \beta_0+\beta_1X_1+ \beta_2X_2 + \beta_3X_3\)</span>.</p>
<p>Puede ser que el mejor modelo sea</p>
<ul>
<li><p><span class="math inline">\(Y = \beta_0\)</span>,</p></li>
<li><p><span class="math inline">\(Y = \beta_0+\beta_1X_1\)</span>,</p></li>
<li><p><span class="math inline">\(Y = \beta_0+\beta_2X_2\)</span>,</p></li>
<li><p><span class="math inline">\(Y = \beta_0+\beta_3X_3\)</span>,</p></li>
<li><p><span class="math inline">\(Y = \beta_0+\beta_1X_1+\beta_2X_2\)</span>,</p></li>
<li><p><span class="math inline">\(Y = \beta_0+\beta_1X_1+\beta_3X_3\)</span>, entre otras.</p></li>
</ul>
<p>De los que tienen <span class="math inline">\(k=1\)</span> variable, hay <span class="math inline">\(\binom{3}{1}\)</span> = 3 modelos. Para <span class="math inline">\(k=2\)</span>, son <span class="math inline">\(\binom{3}{2}\)</span> = 3, y para <span class="math inline">\(k=3\)</span>, solo un modelo. Para <span class="math inline">\(k=1\)</span>, se ajustan los 3 y al mejor se le llama <span class="math inline">\(M_1\)</span>. Así para los otros <span class="math inline">\(k\)</span>. Obtenidos estos modelos, se escoge el que tenga la mejor medida, con respecto a los errores antes mencionados.</p>
<p><em>Notas:</em></p>
<ul>
<li>La parte 2.b. se hace con la muestra de entrenamiento. <strong>Objetivo: Minimizar el error de entrenamiento.</strong></li>
<li>La parte 3 se selecciona con los datos de prueba. <strong>Objetivo: Minimizar el error de prueba</strong>.</li>
<li>Si se usa el <span class="math inline">\(RSS\)</span> o <span class="math inline">\(R^2\)</span>, siempre se selecciona el modelo con el número mayor de variables. Esto puede provocar sobreajuste.</li>
<li>El principal inconveniente de este método es que es computacionalmente ineficiente cuando <span class="math inline">\(p\)</span> es relativamente grande.</li>
</ul>
</div>
<div id="selección-de-modelos-hacia-adelante-forward-stepwise-selection" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Selección de modelos hacia adelante (<strong>Forward Stepwise Selection</strong>)<a href="06-seleccion-de-variables.html#selección-de-modelos-hacia-adelante-forward-stepwise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Algoritmo:</em></p>
<ol style="list-style-type: decimal">
<li>Sea <span class="math inline">\(M_0\)</span> el modelo nulo.</li>
<li>Para <span class="math inline">\(k=0,1,\dots,p-1\)</span>,
<ol style="list-style-type: lower-alpha">
<li>Considere los <span class="math inline">\(p-k\)</span> modelos que contenga los predictores en <span class="math inline">\(M_k\)</span> con un predictor adicional.</li>
<li>Seleccione el mejor entre esos <span class="math inline">\(p-k\)</span> modelos usando el <span class="math inline">\(R^2\)</span> o <span class="math inline">\(RSS\)</span>. Llámelo <span class="math inline">\(M_{k+1}\)</span>.</li>
</ol></li>
<li>Seleccione el mejor modelo entre <span class="math inline">\(M_0,\dots, M_p\)</span> usando <span class="math inline">\(CV\)</span>, <span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span> o <span class="math inline">\(R^2\)</span> ajustado.</li>
</ol>
<p><em>Ejemplo:</em> <span class="math inline">\(Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3\)</span></p>
<ul>
<li><p><span class="math inline">\(M_0\)</span>: <span class="math inline">\(Y = \beta_0\)</span></p></li>
<li><p><span class="math inline">\(M_1\)</span>: <span class="math inline">\(Y = \beta_0+\beta_1X_1\)</span>, <span class="math inline">\(Y = \beta_0+\beta_2X_2\)</span> o <span class="math inline">\(Y = \beta_0+\beta_3X_3\)</span>. De los tres se escoge el mejor (por ejemplo, el segundo) y se le llama <span class="math inline">\(M_1\)</span>.</p></li>
<li><p><span class="math inline">\(M_2\)</span>: a <span class="math inline">\(Y = \beta_0+\beta_2X_2\)</span>, que es <span class="math inline">\(M_1\)</span>, se le suma una variable extra (<span class="math inline">\(\beta_1X_1\)</span> o <span class="math inline">\(\beta_3X_3\)</span>) y se selecciona el mejor.</p></li>
<li><p><span class="math inline">\(M_3\)</span>: <span class="math inline">\(M_2\)</span> más la variable no incluida.</p></li>
</ul>
<p><em>Nota:</em></p>
<ul>
<li><p>El número de modelos por calcular usando el mejor subconjunto es <span class="math inline">\(2^p\)</span>, mientras que usando Forward Selection es <span class="math inline">\(1+\displaystyle\sum_0^ {p-1} p-k = \dfrac{1+p(1+p)}2\)</span>.</p></li>
<li><p>No hay garantía de que el modelo seleccionado con este método sea el mejor de todos los posibles.</p></li>
</ul>
</div>
<div id="selección-de-modelos-hacia-atrás-backward-stepwise-selection" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Selección de modelos hacia atrás (<strong>Backward Stepwise Selection</strong>)<a href="06-seleccion-de-variables.html#selección-de-modelos-hacia-atrás-backward-stepwise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Algoritmo:</em></p>
<ol style="list-style-type: decimal">
<li>Sea <span class="math inline">\(M_p\)</span> el modelo completo.</li>
<li>Para <span class="math inline">\(k=p,p-1,\dots,1\)</span>,
<ol style="list-style-type: lower-alpha">
<li>Considere los <span class="math inline">\(k\)</span> modelos que contienen todos excepto uno de los predictores en <span class="math inline">\(M_k\)</span> para un total de <span class="math inline">\(k-1\)</span> predictores.</li>
<li>Seleccione el mejor entre esos <span class="math inline">\(k\)</span> modelos usando el <span class="math inline">\(R^2\)</span> o <span class="math inline">\(RSS\)</span>. Llámelo <span class="math inline">\(M_{k+1}\)</span>.</li>
</ol></li>
<li>Seleccione el mejor modelo entre <span class="math inline">\(M_0,\dots,M_p\)</span> usando <span class="math inline">\(CV\)</span>, <span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span> o <span class="math inline">\(R^2\)</span> ajustado.</li>
</ol>
<p><em>Ejemplo:</em> <span class="math inline">\(Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3\)</span></p>
<ul>
<li><p><span class="math inline">\(M_3\)</span>: <span class="math inline">\(Y = \beta_0 +\beta_1X_1+\beta_2X_2+\beta_3X_3\)</span>.</p></li>
<li><p><span class="math inline">\(M_2\)</span>: se quita una variable (<span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> o <span class="math inline">\(X_3\)</span>) y se selecciona el mejor. Por ejemplo, se remueve <span class="math inline">\(X_1\)</span>.</p></li>
<li><p><span class="math inline">\(M_1\)</span>: A <span class="math inline">\(M_{2}\)</span> le quito otra variable. En este caso, <span class="math inline">\(X_2\)</span> o <span class="math inline">\(X_3\)</span> y se escoge el mejor.</p></li>
<li><p><span class="math inline">\(M_0\)</span>: <span class="math inline">\(Y=\beta_0\)</span>, el modelo nulo.</p></li>
</ul>
<p>Las dos notas del método de Forward Selection aplican para este caso también.</p>
</div>
</div>
<div id="métodos-de-regularización" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Métodos de regularización<a href="06-seleccion-de-variables.html#métodos-de-regularización" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una forma alternativa de hacer selección de variables es a través de la restricción o regularización de los coeficientes de los modelos.</p>
<div id="regresión-ridge" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Regresión Ridge<a href="06-seleccion-de-variables.html#regresión-ridge" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Considere el problema de mínimos cuadrados en el modelo lineal:</p>
<p><span class="math display">\[ RSS = \sum_{i=1}^{n}\left(y_i-\sum_{j=1}^{p}\beta_jX_{ij}\right)^2 \]</span>
y
<span class="math display">\[
\hat\beta = \underset{\beta}{\mathrm{argmin}} RSS
\]</span></p>
<p>La regresión Ridge consiste en determinar</p>
<p><span class="math display">\[ \hat\beta^R_\lambda = \underset{\beta}{\mathrm{argmin}}\left[RSS + \lambda\|\beta_{-0}\|^2_2 \right]\]</span></p>
<p>donde:
<span class="math display">\[\|\beta_{-0}\|^2_2 = \sum_{j=1}^{p}\beta_j^2\]</span>
¿Cómo se debe seleccionar el <span class="math inline">\(\lambda\)</span>?
El método para seleccionarlo es por validación cruzada</p>
<p><em>Nota:</em></p>
<ul>
<li>Si <span class="math inline">\(\lambda = 0\)</span>, <span class="math inline">\(\hat\beta = \beta^R_\lambda\)</span>: caso de máxima varianza, con el menor sesgo posible.</li>
<li>Si <span class="math inline">\(\lambda \to +\infty\)</span>, <span class="math inline">\(\beta \to 0\)</span>: se sacrifican todos los parámetros <span class="math inline">\(\beta\)</span>. Máximo sesgo pero varianza nula.</li>
<li>Cuando <span class="math inline">\(p\)</span> es cercano a <span class="math inline">\(n\)</span>, la varianza a nivel de parámetros es alta (matriz <span class="math inline">\(X\)</span> deja de comportarse como de rango completo). En ese caso la regresión ridge se convierte en una solución para disminuir esa varianza.</li>
<li>Si <span class="math inline">\(p&gt;n\)</span> (mayor cantidad de variables que observaciones), al realizar mínimos cuadrados, no existe una solución única, pero con la forma de regresión de Ridge es posible alcanzarla para un cierto valor de <span class="math inline">\(\lambda\)</span>.</li>
<li>La regresión ridge tiene ventajas computacionales comparado con el método de selección de todos los subconjuntos.</li>
<li>Los estimadores de ridge no son invariantes a cambios de escala (transformaciones <span class="math inline">\(cX_i\)</span>), a diferencia de los estimadores por mínimos cuadrados. Por lo tanto se recomienda estandarizar todas los predictores antes de aplicar ridge.</li>
</ul>
</div>
<div id="regresión-lasso" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Regresión Lasso<a href="06-seleccion-de-variables.html#regresión-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La regresión ridge tiene el inconveniente de que incluye todas los predictores en el modelo, lo cual puede ser un inconveniente para efectos interpretativos (de parámetros) pero no tanto para efectos predictivos, especialmente cuando el número de predictores es alto. Para solucionar este problema se plantea:</p>
<p><span class="math display">\[ \beta_{\lambda}^{LASSO} = \underset{\beta}{\mathrm{argmin}}\left(RSS + \lambda\|\beta_{-0}\|_1 \right)\]</span>
donde:
<span class="math display">\[ \|\beta_{-0}\|_1 = \sum_{j=1}^p|\beta_j|\]</span>
Otra formulación para los métodos vistos son:</p>
<ol style="list-style-type: decimal">
<li>: <span class="math inline">\(\underset{\beta}{\min} RSS\)</span>, sujeto a <span class="math inline">\(\displaystyle\sum_{j=1}^p\beta_j^2 \leq s\)</span>.</li>
<li>: <span class="math inline">\(\underset{\beta}{\min} RSS\)</span>, sujeto a <span class="math inline">\(\displaystyle\sum_{j=1}^p|\beta_j| \leq s\)</span>.
<!-- 1. \textbf{Mejor subconjunto}: $\underset{\beta}{\min} RSS$, sujeto a $\displaystyle\sum_{j=1}^p I_{\lbrace\beta_j \neq 0\rbrace} \leq s$. --></li>
</ol>
<p><em>Nota:</em> la regresión lasso causa una selección de predictores debido a que <span class="math inline">\(RSS\)</span> suele intersecar la región de penalización en esquinas.</p>
</div>
<div id="explicación-gráfica" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Explicación gráfica<a href="06-seleccion-de-variables.html#explicación-gráfica" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="figure">
<img src="manual_figures/ridge-lasso.png" alt="" />
<p class="caption">Tomado de <a href="https://www.datasklr.com/extensions-of-ols-regression/regularization-and-shrinkage-ridge-lasso-and-elastic-net-regression">DataSklr</a></p>
</div>
<div class="figure">
<img src="manual_figures/ridge-lasso2.png" alt="" />
<p class="caption">Tomado de <a href="https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd">Towards to Data Science</a></p>
</div>
</div>
</div>
<div id="laboratorio-6" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Laboratorio<a href="06-seleccion-de-variables.html#laboratorio-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="cross-validation" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Cross-Validation<a href="06-seleccion-de-variables.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="leave-one-out-cross-validation-loocv" class="section level4 hasAnchor" number="6.4.1.1">
<h4><span class="header-section-number">6.4.1.1</span> Leave-one-out Cross Validation (LOOCV)<a href="06-seleccion-de-variables.html#leave-one-out-cross-validation-loocv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Es posible comparar distintos ajustes de modelos usando cross-validation.</p>
<p>Carguemos la base de datos <code>Auto</code> de <code>ISLR</code>.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="06-seleccion-de-variables.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb306-2"><a href="06-seleccion-de-variables.html#cb306-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Auto&quot;</span>)</span></code></pre></div>
<p>Y ajustamos un modelo entre las millas por galon contra los caballos de fuerza de ciertos vehículos.</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="06-seleccion-de-variables.html#cb307-1" aria-hidden="true" tabindex="-1"></a>glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto)</span>
<span id="cb307-2"><a href="06-seleccion-de-variables.html#cb307-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = mpg ~ horsepower, data = Auto)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -13.5710   -3.2592   -0.3435    2.7630   16.9240  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***
## horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 24.06645)
## 
##     Null deviance: 23819.0  on 391  degrees of freedom
## Residual deviance:  9385.9  on 390  degrees of freedom
## AIC: 2363.3
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>La librería <code>boot</code> tiene funciones para aplicar cross-validation. Por ejemplo:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="06-seleccion-de-variables.html#cb309-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot)</span>
<span id="cb309-2"><a href="06-seleccion-de-variables.html#cb309-2" aria-hidden="true" tabindex="-1"></a>cv.err <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(Auto, glm.fit)</span>
<span id="cb309-3"><a href="06-seleccion-de-variables.html#cb309-3" aria-hidden="true" tabindex="-1"></a>cv.err<span class="sc">$</span>delta</span></code></pre></div>
<pre><code>## [1] 24.23151 24.23114</code></pre>
<p>En particular se puede usar un <code>for</code> para aplicar este mismo procedimiento a múltiples modelos.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="06-seleccion-de-variables.html#cb311-1" aria-hidden="true" tabindex="-1"></a>cv.error.LOOCV <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb311-2"><a href="06-seleccion-de-variables.html#cb311-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb311-3"><a href="06-seleccion-de-variables.html#cb311-3" aria-hidden="true" tabindex="-1"></a>    glm.fit <span class="ot">=</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, i), <span class="at">data =</span> Auto)</span>
<span id="cb311-4"><a href="06-seleccion-de-variables.html#cb311-4" aria-hidden="true" tabindex="-1"></a>    cv.error.LOOCV[i] <span class="ot">=</span> <span class="fu">cv.glm</span>(Auto, glm.fit)<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb311-5"><a href="06-seleccion-de-variables.html#cb311-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb311-6"><a href="06-seleccion-de-variables.html#cb311-6" aria-hidden="true" tabindex="-1"></a>cv.error.LOOCV</span></code></pre></div>
<pre><code>## [1] 24.23151 19.24821 19.33498 19.42443 19.03321</code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="06-seleccion-de-variables.html#cb313-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.error.LOOCV, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-206-1.svg" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="k-fold-cross-validation" class="section level4 hasAnchor" number="6.4.1.2">
<h4><span class="header-section-number">6.4.1.2</span> K-Fold Cross Validation<a href="06-seleccion-de-variables.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Este procedimiento se puede repetir con los el K-fold.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="06-seleccion-de-variables.html#cb314-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17</span>)</span>
<span id="cb314-2"><a href="06-seleccion-de-variables.html#cb314-2" aria-hidden="true" tabindex="-1"></a>cv.error<span class="fl">.10</span> <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb314-3"><a href="06-seleccion-de-variables.html#cb314-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb314-4"><a href="06-seleccion-de-variables.html#cb314-4" aria-hidden="true" tabindex="-1"></a>    glm.fit <span class="ot">=</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, i), <span class="at">data =</span> Auto)</span>
<span id="cb314-5"><a href="06-seleccion-de-variables.html#cb314-5" aria-hidden="true" tabindex="-1"></a>    cv.error<span class="fl">.10</span>[i] <span class="ot">=</span> <span class="fu">cv.glm</span>(Auto, glm.fit, <span class="at">K =</span> <span class="dv">10</span>)<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb314-6"><a href="06-seleccion-de-variables.html#cb314-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb314-7"><a href="06-seleccion-de-variables.html#cb314-7" aria-hidden="true" tabindex="-1"></a>cv.error<span class="fl">.10</span></span></code></pre></div>
<pre><code>##  [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666
##  [9] 18.87013 20.95520</code></pre>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="06-seleccion-de-variables.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.error.LOOCV, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb316-2"><a href="06-seleccion-de-variables.html#cb316-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(cv.error<span class="fl">.10</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-207-1.svg" width="70%" style="display: block; margin: auto;" />
### Selección de variables</p>
<p>Cargue los datos <code>Hitters</code> del paquete <code>ISLR</code> que representan el salario de varios jugadores de beisbol y sus estadística de juego (número de bateos, home runs, carreras, etc.).</p>
</div>
<div id="análisis-exploratorio" class="section level4 hasAnchor" number="6.4.1.3">
<h4><span class="header-section-number">6.4.1.3</span> Análisis exploratorio<a href="06-seleccion-de-variables.html#análisis-exploratorio" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Con esta información, haga un análisis exploratorio de los datos usando <code>ggpairs</code>.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="06-seleccion-de-variables.html#cb317-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb317-2"><a href="06-seleccion-de-variables.html#cb317-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(Hitters)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-208-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="06-seleccion-de-variables.html#cb318-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Hitters)</span></code></pre></div>
<pre><code>##      AtBat            Hits         HmRun            Runs       
##  Min.   : 16.0   Min.   :  1   Min.   : 0.00   Min.   :  0.00  
##  1st Qu.:255.2   1st Qu.: 64   1st Qu.: 4.00   1st Qu.: 30.25  
##  Median :379.5   Median : 96   Median : 8.00   Median : 48.00  
##  Mean   :380.9   Mean   :101   Mean   :10.77   Mean   : 50.91  
##  3rd Qu.:512.0   3rd Qu.:137   3rd Qu.:16.00   3rd Qu.: 69.00  
##  Max.   :687.0   Max.   :238   Max.   :40.00   Max.   :130.00  
##                                                                
##       RBI             Walks            Years            CAtBat       
##  Min.   :  0.00   Min.   :  0.00   Min.   : 1.000   Min.   :   19.0  
##  1st Qu.: 28.00   1st Qu.: 22.00   1st Qu.: 4.000   1st Qu.:  816.8  
##  Median : 44.00   Median : 35.00   Median : 6.000   Median : 1928.0  
##  Mean   : 48.03   Mean   : 38.74   Mean   : 7.444   Mean   : 2648.7  
##  3rd Qu.: 64.75   3rd Qu.: 53.00   3rd Qu.:11.000   3rd Qu.: 3924.2  
##  Max.   :121.00   Max.   :105.00   Max.   :24.000   Max.   :14053.0  
##                                                                      
##      CHits            CHmRun           CRuns             CRBI        
##  Min.   :   4.0   Min.   :  0.00   Min.   :   1.0   Min.   :   0.00  
##  1st Qu.: 209.0   1st Qu.: 14.00   1st Qu.: 100.2   1st Qu.:  88.75  
##  Median : 508.0   Median : 37.50   Median : 247.0   Median : 220.50  
##  Mean   : 717.6   Mean   : 69.49   Mean   : 358.8   Mean   : 330.12  
##  3rd Qu.:1059.2   3rd Qu.: 90.00   3rd Qu.: 526.2   3rd Qu.: 426.25  
##  Max.   :4256.0   Max.   :548.00   Max.   :2165.0   Max.   :1659.00  
##                                                                      
##      CWalks        League  Division    PutOuts          Assists     
##  Min.   :   0.00   A:175   E:157    Min.   :   0.0   Min.   :  0.0  
##  1st Qu.:  67.25   N:147   W:165    1st Qu.: 109.2   1st Qu.:  7.0  
##  Median : 170.50                    Median : 212.0   Median : 39.5  
##  Mean   : 260.24                    Mean   : 288.9   Mean   :106.9  
##  3rd Qu.: 339.25                    3rd Qu.: 325.0   3rd Qu.:166.0  
##  Max.   :1566.00                    Max.   :1378.0   Max.   :492.0  
##                                                                     
##      Errors          Salary       NewLeague
##  Min.   : 0.00   Min.   :  67.5   A:176    
##  1st Qu.: 3.00   1st Qu.: 190.0   N:146    
##  Median : 6.00   Median : 425.0            
##  Mean   : 8.04   Mean   : 535.9            
##  3rd Qu.:11.00   3rd Qu.: 750.0            
##  Max.   :32.00   Max.   :2460.0            
##                  NA&#39;s   :59</code></pre>
<p>Para limpiar la base de datos de <code>NA</code> usamos <code>dplyr</code>.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="06-seleccion-de-variables.html#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb320-2"><a href="06-seleccion-de-variables.html#cb320-2" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> Hitters <span class="sc">%&gt;%</span></span>
<span id="cb320-3"><a href="06-seleccion-de-variables.html#cb320-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">drop_na</span>()</span></code></pre></div>
<p>Cargue la librería</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="06-seleccion-de-variables.html#cb321-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps)</span></code></pre></div>
<p>y busque la ayuda de la función <code>regsubsets</code>. Use esta función para ajustar todo los posibles modelos de la forma <code>Salary ~ .</code>.</p>
<p>Puede guardar estos modelos en ciertas variables (e.g. <code>regfit.full</code>) y usar la función <code>plot</code>.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="06-seleccion-de-variables.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb322-2"><a href="06-seleccion-de-variables.html#cb322-2" aria-hidden="true" tabindex="-1"></a>regfit.full <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., Hitters, <span class="at">nvmax =</span> <span class="dv">19</span>)</span>
<span id="cb322-3"><a href="06-seleccion-de-variables.html#cb322-3" aria-hidden="true" tabindex="-1"></a>regfit.full.summary <span class="ot">&lt;-</span> <span class="fu">summary</span>(regfit.full)</span>
<span id="cb322-4"><a href="06-seleccion-de-variables.html#cb322-4" aria-hidden="true" tabindex="-1"></a>regfit.full.summary</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., Hitters, nvmax = 19)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 19
## Selection Algorithm: exhaustive
##           AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
## 1  ( 1 )  &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 2  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 3  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 4  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 5  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 6  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 7  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot; 
## 8  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot; 
## 9  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 10  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 11  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 12  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 13  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 14  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 15  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 16  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 17  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 18  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 19  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot; 
##           CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 9  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 10  ( 1 ) &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 11  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 12  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 13  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 14  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 15  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 16  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 17  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 18  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 19  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="06-seleccion-de-variables.html#cb324-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(regfit.full.summary)</span></code></pre></div>
<pre><code>## List of 8
##  $ which : logi [1:19, 1:20] TRUE TRUE TRUE TRUE TRUE TRUE ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:19] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:20] &quot;(Intercept)&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; ...
##  $ rsq   : num [1:19] 0.321 0.425 0.451 0.475 0.491 ...
##  $ rss   : num [1:19] 36179679 30646560 29249297 27970852 27149899 ...
##  $ adjr2 : num [1:19] 0.319 0.421 0.445 0.467 0.481 ...
##  $ cp    : num [1:19] 104.3 50.7 38.7 27.9 21.6 ...
##  $ bic   : num [1:19] -90.8 -128.9 -135.6 -141.8 -144.1 ...
##  $ outmat: chr [1:19, 1:19] &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:19] &quot;1  ( 1 )&quot; &quot;2  ( 1 )&quot; &quot;3  ( 1 )&quot; &quot;4  ( 1 )&quot; ...
##   .. ..$ : chr [1:19] &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; &quot;Runs&quot; ...
##  $ obj   :List of 28
##   ..$ np       : int 20
##   ..$ nrbar    : int 190
##   ..$ d        : num [1:20] 2.63e+02 1.10e+08 1.61e+05 1.85e+07 5.58e+03 ...
##   ..$ rbar     : num [1:190] 722.19 51.49 290.71 11.62 7.31 ...
##   ..$ thetab   : num [1:20] 535.926 0.382 5.509 0.306 -4.051 ...
##   ..$ first    : int 2
##   ..$ last     : int 20
##   ..$ vorder   : int [1:20] 1 10 6 17 4 8 19 16 5 15 ...
##   ..$ tol      : num [1:20] 8.11e-09 1.71e-05 7.91e-07 7.08e-06 1.94e-07 ...
##   ..$ rss      : num [1:20] 53319113 37253973 32381808 30651377 30559801 ...
##   ..$ bound    : num [1:20] 53319113 36179679 30646560 29249297 27970852 ...
##   ..$ nvmax    : int 20
##   ..$ ress     : num [1:20, 1] 53319113 36179679 30646560 29249297 27970852 ...
##   ..$ ir       : int 20
##   ..$ nbest    : int 1
##   ..$ lopt     : int [1:210, 1] 1 1 13 1 3 13 1 3 17 13 ...
##   ..$ il       : int 210
##   ..$ ier      : int 0
##   ..$ xnames   : chr [1:20] &quot;(Intercept)&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; ...
##   ..$ method   : chr &quot;exhaustive&quot;
##   ..$ force.in : Named logi [1:20] TRUE FALSE FALSE FALSE FALSE FALSE ...
##   .. ..- attr(*, &quot;names&quot;)= chr [1:20] &quot;&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; ...
##   ..$ force.out: Named logi [1:20] FALSE FALSE FALSE FALSE FALSE FALSE ...
##   .. ..- attr(*, &quot;names&quot;)= chr [1:20] &quot;&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; ...
##   ..$ sserr    : num 24200700
##   ..$ intercept: logi TRUE
##   ..$ lindep   : logi [1:20] FALSE FALSE FALSE FALSE FALSE FALSE ...
##   ..$ nullrss  : num 53319113
##   ..$ nn       : int 263
##   ..$ call     : language regsubsets.formula(Salary ~ ., Hitters, nvmax = 19)
##   ..- attr(*, &quot;class&quot;)= chr &quot;regsubsets&quot;
##  - attr(*, &quot;class&quot;)= chr &quot;summary.regsubsets&quot;</code></pre>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="06-seleccion-de-variables.html#cb326-1" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="fu">which.max</span>(regfit.full.summary<span class="sc">$</span>adjr2)</span>
<span id="cb326-2"><a href="06-seleccion-de-variables.html#cb326-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.full.summary<span class="sc">$</span>adjr2)</span>
<span id="cb326-3"><a href="06-seleccion-de-variables.html#cb326-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(idx, regfit.full.summary<span class="sc">$</span>adjr2[idx], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb326-4"><a href="06-seleccion-de-variables.html#cb326-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-213-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="06-seleccion-de-variables.html#cb327-1" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(regfit.full.summary<span class="sc">$</span>cp)</span>
<span id="cb327-2"><a href="06-seleccion-de-variables.html#cb327-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.full.summary<span class="sc">$</span>cp)</span>
<span id="cb327-3"><a href="06-seleccion-de-variables.html#cb327-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(idx, regfit.full.summary<span class="sc">$</span>cp[idx], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb327-4"><a href="06-seleccion-de-variables.html#cb327-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-214-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="06-seleccion-de-variables.html#cb328-1" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(regfit.full.summary<span class="sc">$</span>bic)</span>
<span id="cb328-2"><a href="06-seleccion-de-variables.html#cb328-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.full.summary<span class="sc">$</span>bic)</span>
<span id="cb328-3"><a href="06-seleccion-de-variables.html#cb328-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(idx, regfit.full.summary<span class="sc">$</span>bic[idx], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb328-4"><a href="06-seleccion-de-variables.html#cb328-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-215-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="06-seleccion-de-variables.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.full, <span class="at">scale =</span> <span class="st">&quot;bic&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-216-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="06-seleccion-de-variables.html#cb330-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.full, <span class="at">scale =</span> <span class="st">&quot;Cp&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-216-2.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="06-seleccion-de-variables.html#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.full, <span class="at">scale =</span> <span class="st">&quot;adjr2&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-216-3.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="06-seleccion-de-variables.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(regfit.full, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns 
##  162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 
##         CRBI       CWalks    DivisionW      PutOuts      Assists 
##    0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680</code></pre>
</div>
<div id="regresión-forward-y-backward" class="section level4 hasAnchor" number="6.4.1.4">
<h4><span class="header-section-number">6.4.1.4</span> Regresión forward y backward<a href="06-seleccion-de-variables.html#regresión-forward-y-backward" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La función <code>regsubsets</code> tiene un paramétro <code>method</code>. Usen los valores <code>forward</code> y <code>backward</code> y comparen los resultados.</p>
<p>Puede guardar estos modelos en ciertas variables (e.g. <code>regfit.fwd</code> y <code>regfit.bwd</code>) y usar la función <code>plot</code>.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="06-seleccion-de-variables.html#cb334-1" aria-hidden="true" tabindex="-1"></a>regfit.fwd <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters,</span>
<span id="cb334-2"><a href="06-seleccion-de-variables.html#cb334-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">nvmax =</span> <span class="dv">19</span>, <span class="at">method =</span> <span class="st">&quot;forward&quot;</span>)</span>
<span id="cb334-3"><a href="06-seleccion-de-variables.html#cb334-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(regfit.fwd)</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = &quot;forward&quot;)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 19
## Selection Algorithm: forward
##           AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
## 1  ( 1 )  &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 2  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 3  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 4  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 5  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 6  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 7  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 8  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 9  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 10  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 11  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 12  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 13  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 14  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 15  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 16  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 17  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 18  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 19  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot; 
##           CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 9  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 10  ( 1 ) &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 11  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 12  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 13  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 14  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 15  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 16  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 17  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 18  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 19  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="06-seleccion-de-variables.html#cb336-1" aria-hidden="true" tabindex="-1"></a>regfit.bwd <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters,</span>
<span id="cb336-2"><a href="06-seleccion-de-variables.html#cb336-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">nvmax =</span> <span class="dv">19</span>, <span class="at">method =</span> <span class="st">&quot;backward&quot;</span>)</span>
<span id="cb336-3"><a href="06-seleccion-de-variables.html#cb336-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(regfit.bwd)</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = &quot;backward&quot;)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 19
## Selection Algorithm: backward
##           AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
## 1  ( 1 )  &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot; &quot; 
## 2  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot; &quot; 
## 3  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot; &quot; 
## 4  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot; &quot; 
## 5  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot; &quot; 
## 6  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot; &quot; 
## 7  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot; &quot; 
## 8  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 9  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 10  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 11  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 12  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 13  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 14  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 15  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 16  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 17  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 18  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 19  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot; 
##           CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 9  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 10  ( 1 ) &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 11  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 12  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 13  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 14  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 15  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 16  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 17  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 18  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 19  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;</code></pre>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="06-seleccion-de-variables.html#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb338-2"><a href="06-seleccion-de-variables.html#cb338-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.fwd, <span class="at">scale =</span> <span class="st">&quot;bic&quot;</span>)</span>
<span id="cb338-3"><a href="06-seleccion-de-variables.html#cb338-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.bwd, <span class="at">scale =</span> <span class="st">&quot;bic&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-219-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="06-seleccion-de-variables.html#cb339-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb339-2"><a href="06-seleccion-de-variables.html#cb339-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.fwd, <span class="at">scale =</span> <span class="st">&quot;Cp&quot;</span>)</span>
<span id="cb339-3"><a href="06-seleccion-de-variables.html#cb339-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.bwd, <span class="at">scale =</span> <span class="st">&quot;Cp&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-220-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="06-seleccion-de-variables.html#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb340-2"><a href="06-seleccion-de-variables.html#cb340-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.fwd, <span class="at">scale =</span> <span class="st">&quot;adjr2&quot;</span>)</span>
<span id="cb340-3"><a href="06-seleccion-de-variables.html#cb340-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(regfit.bwd, <span class="at">scale =</span> <span class="st">&quot;adjr2&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-221-1.svg" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="regresión-ridge-1" class="section level4 hasAnchor" number="6.4.1.5">
<h4><span class="header-section-number">6.4.1.5</span> Regresión Ridge<a href="06-seleccion-de-variables.html#regresión-ridge-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="06-seleccion-de-variables.html#cb341-1" aria-hidden="true" tabindex="-1"></a>mm <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., Hitters)[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb341-2"><a href="06-seleccion-de-variables.html#cb341-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> mm[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb341-3"><a href="06-seleccion-de-variables.html#cb341-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mm[, <span class="dv">1</span>]</span></code></pre></div>
<p>Usando el paquete <code>glmnet</code> y la función con el mismo nombre, ejecute el siguiente comando</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="06-seleccion-de-variables.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb342-2"><a href="06-seleccion-de-variables.html#cb342-2" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb342-3"><a href="06-seleccion-de-variables.html#cb342-3" aria-hidden="true" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid)</span></code></pre></div>
<p>El factor <code>lambda</code> representa el <span class="math inline">\(\lambda\)</span> de la fórmula</p>
<p><span class="math display">\[ \hat{\beta} = \underset{\beta}{\mathrm{argmin}} \left\{RSS + \lambda \Vert \beta \Vert_2^2\right\}.\]</span></p>
<p>Si no se incluye el paramétro lambda del modelo, R construye una secuencia de <span class="math inline">\(\lambda&#39;\)</span>s estimados por validación cruzada.</p>
<p>Haga lo siguiente:</p>
<ol style="list-style-type: decimal">
<li><p>Construya un modelo usando todos los datos (sin separar muestra de entrenamiento y prueba).</p></li>
<li><p>Construya el siguiente modelo</p></li>
</ol>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="06-seleccion-de-variables.html#cb343-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb343-2"><a href="06-seleccion-de-variables.html#cb343-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x), <span class="fu">nrow</span>(x)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb343-3"><a href="06-seleccion-de-variables.html#cb343-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="sc">-</span>train</span>
<span id="cb343-4"><a href="06-seleccion-de-variables.html#cb343-4" aria-hidden="true" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> y[test]</span>
<span id="cb343-5"><a href="06-seleccion-de-variables.html#cb343-5" aria-hidden="true" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb343-6"><a href="06-seleccion-de-variables.html#cb343-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">lambda =</span> grid)</span>
<span id="cb343-7"><a href="06-seleccion-de-variables.html#cb343-7" aria-hidden="true" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">4</span>, <span class="at">newx =</span> x[test,</span>
<span id="cb343-8"><a href="06-seleccion-de-variables.html#cb343-8" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb343-9"><a href="06-seleccion-de-variables.html#cb343-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb343-10"><a href="06-seleccion-de-variables.html#cb343-10" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE</span></span>
<span id="cb343-11"><a href="06-seleccion-de-variables.html#cb343-11" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1518.103</code></pre>
<p>¿Qué ocurre si se cambia el paramétro <span class="math inline">\(s\)</span> de <code>predict</code> por un <code>10e10</code> (i.e. <span class="math inline">\(10^{10}\)</span>). Comente los resultados. ¿Y qué ocurre si <span class="math inline">\(s=0\)</span>?</p>
<ol start="3" style="list-style-type: decimal">
<li>Finalmente, ejecute el siguiente código</li>
</ol>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="06-seleccion-de-variables.html#cb345-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb345-2"><a href="06-seleccion-de-variables.html#cb345-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb345-3"><a href="06-seleccion-de-variables.html#cb345-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-225-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<p>Busque la ayuda de <code>cv.glmnet</code> y deduzca qué significa el gráfico.</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="06-seleccion-de-variables.html#cb346-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb346-2"><a href="06-seleccion-de-variables.html#cb346-2" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb346-3"><a href="06-seleccion-de-variables.html#cb346-3" aria-hidden="true" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid)</span></code></pre></div>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="06-seleccion-de-variables.html#cb347-1" aria-hidden="true" tabindex="-1"></a>ridge.mod<span class="sc">$</span>lambda[<span class="dv">50</span>]</span></code></pre></div>
<pre><code>## [1] 11497.57</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="06-seleccion-de-variables.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge.mod)[, <span class="dv">50</span>]</span></code></pre></div>
<pre><code>##   (Intercept)          Hits         HmRun          Runs           RBI 
##  3.863561e+02  3.839911e-02  1.116115e-01  6.308357e-02  5.469586e-02 
##         Walks         Years        CAtBat         CHits        CHmRun 
##  5.096121e-02 -5.083625e-04  1.512925e-04  5.845613e-04  4.270574e-03 
##         CRuns          CRBI        CWalks       LeagueN     DivisionW 
##  1.203919e-03  1.138186e-03  7.896665e-04 -5.373933e-01 -1.929369e-01 
##       PutOuts       Assists        Errors    NewLeagueN 
##  1.962248e-03  4.290830e-03  8.913766e-02 -3.078976e-01</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="06-seleccion-de-variables.html#cb351-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge.mod)[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">50</span>]<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.6725377</code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="06-seleccion-de-variables.html#cb353-1" aria-hidden="true" tabindex="-1"></a>ridge.mod<span class="sc">$</span>lambda[<span class="dv">60</span>]</span></code></pre></div>
<pre><code>## [1] 705.4802</code></pre>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="06-seleccion-de-variables.html#cb355-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(ridge.mod)[, <span class="dv">60</span>]</span></code></pre></div>
<pre><code>##   (Intercept)          Hits         HmRun          Runs           RBI 
##  2.533802e+02  3.853492e-01  8.799062e-01  6.027728e-01  4.892400e-01 
##         Walks         Years        CAtBat         CHits        CHmRun 
##  4.379322e-01 -3.131147e-01  8.266820e-04  3.442277e-03  1.593816e-02 
##         CRuns          CRBI        CWalks       LeagueN     DivisionW 
##  6.793465e-03  5.273965e-03 -1.061786e-04 -4.147140e+00 -9.052165e-01 
##       PutOuts       Assists        Errors    NewLeagueN 
##  1.659389e-02  4.579147e-02  8.721363e-01 -1.537659e+00</code></pre>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="06-seleccion-de-variables.html#cb357-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge.mod)[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">60</span>]<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 4.791781</code></pre>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="06-seleccion-de-variables.html#cb359-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge.mod)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-229-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="06-seleccion-de-variables.html#cb360-1" aria-hidden="true" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span></code></pre></div>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="06-seleccion-de-variables.html#cb361-1" aria-hidden="true" tabindex="-1"></a>ridge.mod<span class="sc">$</span>lambda</span></code></pre></div>
<pre><code>##   [1] 141729.38335 129138.53556 117666.22398 107213.08094  97688.56632
##   [6]  89010.18333  81102.76397  73897.81795  67332.93947  61351.26671
##  [11]  55900.98926  50934.89945  46409.98336  42287.04835  38530.38352
##  [16]  35107.45045  31988.60131  29146.82213  26557.49879  24198.20379
##  [21]  22048.50206  20089.77390  18305.05376  16678.88323  15197.17722
##  [26]  13847.10188  12616.96351  11496.10725  10474.82476   9544.27020
##  [31]   8696.38354   7923.82080   7219.89040   6578.49523   5994.07985
##  [36]   5461.58231   4976.39039   4534.30159   4131.48673   3764.45684
##  [41]   3430.03287   3125.31820   2847.67354   2594.69408   2364.18861
##  [46]   2154.16061   1962.79091   1788.42198   1629.54350   1484.77935
##  [51]   1352.87564   1232.68990   1123.18113   1023.40081    932.48470
##  [56]    849.64533    774.16518    705.39048    642.72553    585.62757
##  [61]    533.60203    486.19830    443.00578    403.65037    367.79118
##  [66]    335.11763    305.34670    278.22054    253.50419    230.98358
##  [71]    210.46364    191.76663    174.73061    159.20802    145.06442
##  [76]    132.17730    120.43503    109.73591     99.98728     91.10468
##  [81]     83.01119     75.63671     68.91735     62.79492     57.21640
##  [86]     52.13345     47.50206     43.28211     39.43704     35.93356
##  [91]     32.74133     29.83268     27.18242     24.76761     22.56733
##  [96]     20.56251     18.73579     17.07135     15.55478     14.17294</code></pre>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="06-seleccion-de-variables.html#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge.mod)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-230-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="06-seleccion-de-variables.html#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">50</span>, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>)</span></code></pre></div>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s1
## (Intercept) 94.5913466484
## Hits         1.1742395636
## HmRun        0.5429246143
## Runs         1.3628990314
## RBI          0.9229879932
## Walks        0.6114551657
## Years       -0.9130962202
## CAtBat       0.0022266078
## CHits        0.0071572845
## CHmRun      -0.0190970399
## CRuns        0.0101467149
## CRBI         0.0006507061
## CWalks      -0.0293738133
## LeagueN     -7.1549920715
## DivisionW    5.0662991581
## PutOuts      0.0207351205
## Assists      0.1035681348
## Errors       1.1324147256
## NewLeagueN   4.8784587235</code></pre>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="06-seleccion-de-variables.html#cb366-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb366-2"><a href="06-seleccion-de-variables.html#cb366-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x), <span class="fu">nrow</span>(x)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb366-3"><a href="06-seleccion-de-variables.html#cb366-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="sc">-</span>train</span>
<span id="cb366-4"><a href="06-seleccion-de-variables.html#cb366-4" aria-hidden="true" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> y[test]</span>
<span id="cb366-5"><a href="06-seleccion-de-variables.html#cb366-5" aria-hidden="true" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb366-6"><a href="06-seleccion-de-variables.html#cb366-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">lambda =</span> grid)</span>
<span id="cb366-7"><a href="06-seleccion-de-variables.html#cb366-7" aria-hidden="true" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict.glmnet</span>(ridge.mod, <span class="at">s =</span> <span class="dv">4</span>, <span class="at">newx =</span> x[test,</span>
<span id="cb366-8"><a href="06-seleccion-de-variables.html#cb366-8" aria-hidden="true" tabindex="-1"></a>    ], <span class="at">exact =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="06-seleccion-de-variables.html#cb367-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE</span></span>
<span id="cb367-2"><a href="06-seleccion-de-variables.html#cb367-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1518.103</code></pre>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="06-seleccion-de-variables.html#cb369-1" aria-hidden="true" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="fl">1e+10</span>, <span class="at">newx =</span> x[test,</span>
<span id="cb369-2"><a href="06-seleccion-de-variables.html#cb369-2" aria-hidden="true" tabindex="-1"></a>    ], <span class="at">exact =</span> <span class="cn">TRUE</span>)</span>
<span id="cb369-3"><a href="06-seleccion-de-variables.html#cb369-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 21582.12</code></pre>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="06-seleccion-de-variables.html#cb371-1" aria-hidden="true" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">0</span>, <span class="at">newx =</span> x[test,</span>
<span id="cb371-2"><a href="06-seleccion-de-variables.html#cb371-2" aria-hidden="true" tabindex="-1"></a>    ], <span class="at">exact =</span> <span class="cn">FALSE</span>)</span>
<span id="cb371-3"><a href="06-seleccion-de-variables.html#cb371-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1806.907</code></pre>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="06-seleccion-de-variables.html#cb373-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">subset =</span> train)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, subset = train)
## 
## Coefficients:
## (Intercept)        xHits       xHmRun        xRuns         xRBI       xWalks  
##    55.76031      2.98008     -0.30105     -0.44890      0.29740      0.78322  
##      xYears      xCAtBat       xCHits      xCHmRun       xCRuns        xCRBI  
##    -2.98949      0.11920     -0.46305     -0.04969      0.07318      0.13056  
##     xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists      xErrors  
##    -0.13227     -9.74183      1.03070      0.00804      0.04660      0.65019  
## xNewLeagueN  
##     5.53145</code></pre>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="06-seleccion-de-variables.html#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">0</span>, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">exact =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       s1
## (Intercept) 56.400164521
## Hits         2.935963537
## HmRun       -0.411469189
## Runs        -0.412389832
## RBI          0.355794523
## Walks        0.770020095
## Years       -2.741424760
## CAtBat       0.109568413
## CHits       -0.417184445
## CHmRun       0.025750294
## CRuns        0.059026090
## CRBI         0.100039043
## CWalks      -0.128465225
## LeagueN     -8.986012754
## DivisionW    1.414164932
## PutOuts      0.008851545
## Assists      0.050791034
## Errors       0.647146833
## NewLeagueN   4.525279141</code></pre>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="06-seleccion-de-variables.html#cb377-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb377-2"><a href="06-seleccion-de-variables.html#cb377-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb377-3"><a href="06-seleccion-de-variables.html#cb377-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb377-4"><a href="06-seleccion-de-variables.html#cb377-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-232-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="06-seleccion-de-variables.html#cb378-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb378-2"><a href="06-seleccion-de-variables.html#cb378-2" aria-hidden="true" tabindex="-1"></a>bestlam</span></code></pre></div>
<pre><code>## [1] 14.27118</code></pre>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="06-seleccion-de-variables.html#cb380-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(bestlam)</span></code></pre></div>
<pre><code>## [1] 2.658242</code></pre>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="06-seleccion-de-variables.html#cb382-1" aria-hidden="true" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> bestlam, <span class="at">newx =</span> x[test,</span>
<span id="cb382-2"><a href="06-seleccion-de-variables.html#cb382-2" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb382-3"><a href="06-seleccion-de-variables.html#cb382-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1669.854</code></pre>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="06-seleccion-de-variables.html#cb384-1" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb384-2"><a href="06-seleccion-de-variables.html#cb384-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(out, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">s =</span> bestlam, <span class="at">exact =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       s1
## (Intercept) 69.824252126
## Hits         1.586649928
## HmRun       -0.083817620
## Runs         1.298201495
## RBI          0.885217905
## Walks        0.521415084
## Years       -0.897115251
## CAtBat       0.004160358
## CHits        0.005192629
## CHmRun       0.022521114
## CRuns        0.009343228
## CRBI        -0.008077328
## CWalks      -0.045652211
## LeagueN     -8.954990325
## DivisionW    7.576655558
## PutOuts      0.016433884
## Assists      0.098673953
## Errors       0.806331327
## NewLeagueN   8.465527964</code></pre>
</div>
</div>
<div id="regresión-lasso-1" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Regresión Lasso<a href="06-seleccion-de-variables.html#regresión-lasso-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ejecute los procedimientos anteriores con glmnet pero modifique el paramétro <code>alpha = 0</code>. Compare los resultados.</p>
<p>En este caso, se están encontrando los valores de <span class="math inline">\(\beta\)</span> tal que
<span class="math display">\[\hat{\beta} = \underset{\beta}{\mathrm{argmin}} \left\{RSS + \lambda \Vert \beta \Vert_1^2\right\}.\]</span></p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="06-seleccion-de-variables.html#cb386-1" aria-hidden="true" tabindex="-1"></a>lasso.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb386-2"><a href="06-seleccion-de-variables.html#cb386-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">lambda =</span> grid)</span>
<span id="cb386-3"><a href="06-seleccion-de-variables.html#cb386-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.mod)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-233-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="06-seleccion-de-variables.html#cb387-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb387-2"><a href="06-seleccion-de-variables.html#cb387-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb387-3"><a href="06-seleccion-de-variables.html#cb387-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-234-1.svg" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="06-seleccion-de-variables.html#cb388-1" aria-hidden="true" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb388-2"><a href="06-seleccion-de-variables.html#cb388-2" aria-hidden="true" tabindex="-1"></a>bestlam</span></code></pre></div>
<pre><code>## [1] 0.06939507</code></pre>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="06-seleccion-de-variables.html#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(bestlam)</span></code></pre></div>
<pre><code>## [1] -2.667939</code></pre>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="06-seleccion-de-variables.html#cb392-1" aria-hidden="true" tabindex="-1"></a>lasso.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso.mod, <span class="at">s =</span> bestlam, <span class="at">newx =</span> x[test,</span>
<span id="cb392-2"><a href="06-seleccion-de-variables.html#cb392-2" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb392-3"><a href="06-seleccion-de-variables.html#cb392-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((lasso.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1698.516</code></pre>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="06-seleccion-de-variables.html#cb394-1" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid)</span>
<span id="cb394-2"><a href="06-seleccion-de-variables.html#cb394-2" aria-hidden="true" tabindex="-1"></a>lasso.coef <span class="ot">&lt;-</span> <span class="fu">predict</span>(out, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">s =</span> bestlam)</span>
<span id="cb394-3"><a href="06-seleccion-de-variables.html#cb394-3" aria-hidden="true" tabindex="-1"></a>lasso.coef</span></code></pre></div>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       s1
## (Intercept)  49.43034834
## Hits          2.58756283
## HmRun         .         
## Runs          0.01352442
## RBI           0.50950339
## Walks         0.73008990
## Years        -2.72097480
## CAtBat        0.06956598
## CHits        -0.22930533
## CHmRun        0.10795338
## CRuns         0.11833697
## CRBI         -0.05929447
## CWalks       -0.11751099
## LeagueN     -10.52682373
## DivisionW     6.41207311
## PutOuts       0.01253616
## Assists       0.04384029
## Errors        0.79511992
## NewLeagueN   10.34406169</code></pre>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="06-seleccion-de-variables.html#cb396-1" aria-hidden="true" tabindex="-1"></a>lasso.coef[lasso.coef <span class="sc">!=</span> <span class="dv">0</span>]</span></code></pre></div>
<pre><code>##  [1]  49.43034834   2.58756283   0.01352442   0.50950339   0.73008990
##  [6]  -2.72097480   0.06956598  -0.22930533   0.10795338   0.11833697
## [11]  -0.05929447  -0.11751099 -10.52682373   6.41207311   0.01253616
## [16]   0.04384029   0.79511992  10.34406169</code></pre>
</div>
</div>
<div id="ejercicios-4" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Ejercicios<a href="06-seleccion-de-variables.html#ejercicios-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Del libro <span class="citation">(James et al. 2013)</span>
<ul>
<li>Capítulo 5. 2, 5, 8.</li>
<li>Capítulo 6: 5, 6, 7, 8, 10.</li>
</ul></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="05-regresion-logistica.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="referencias.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica/edit/master/06-seleccion-de-variables.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica/blob/master/06-seleccion-de-variables.Rmd",
"text": null
},
"download": ["Notas-Curso-Estadistica.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toc_depth": 5
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
