# Análisis en componentes principales

## Representación gráfica

![Tomado de [The shape of data](https://shapeofdata.wordpress.com/2013/04/09/principle-component-analysis/)](manual_figures/pca.png)

## Aprendizaje no-supervisado

Al contrario de los métodos que se han estudiado de regresión y clasificación, en este caso no hay variable dependiente, y el conjunto de datos está compuesto de $p$ variables o características y $n$ observaciones. 

El principal objetivo del aprendizaje no-supervisado no es la predicción, sino en el *análisis de datos* por sí mismo, es decir se quiere buscar patrones o relaciones interesantes dentro de la tabla de datos: por ejemplo la visualización de datos o la identificación de subgrupos en los datos. (Análisis Exploratorio de Datos)

En el caso de aprendizaje no-supervisado, no es posible verificar o validar los métodos adoptados.


Si se quiere seleccionar la mejor proyección de 2 variables de una nube de puntos $X_1,\dots, X_p$, se debe hacer $\binom{p}{2}$ gráficos de dispersión. Un criterio de búsqueda es seleccionar la que tenga mayor información, en el sentido de mayor variabilidad.

Usaremos como base los libros de [@HussonExploratory2017] y [@James2013b].

```{r}
library(rgl)
library(car)
knitr::knit_hooks$set(webgl = hook_webgl, rgl = hook_rgl)
knitr::opts_chunk$set(fig.pos = "!h")
```

```{r}
set.seed(123)
x1 <- rnorm(1000, 0, 1)
x2 <- x1 + rnorm(1000, 0, 0.5)
x3 <- cos(runif(1000, -3, 3))
```

```{r}
GGally::ggpairs(data.frame(x1, x2, x3))
```


```{r,webgl=TRUE,echo=knitr::is_html_output()}
plot3d(x1, x2, x3, point.col = "black")
plot3d(scale(x1), scale(x2), scale(x3), point.col = "black")
```

```{r,rgl=TRUE,echo=knitr::is_latex_output()}
plot3d(x1, x3, x2, point.col = "black")
plot3d(scale(x1), scale(x2), scale(x3), point.col = "black")
```




<!-- ```{r, echo=FALSE} -->
<!-- library(ggplot2) -->

<!-- qplot(1:300, rnorm(300, sd = 0.1) + 5, ylim = c(0, 10)) + theme_minimal() + xlab("") + ylab("") -->
<!-- qplot(1:300, (3 * 1:300 + 100 * cos(1:300 / (2 * pi)) + 200 * rnorm(300, sd = -->
<!--                                                                       0.1)) / 100) + theme_minimal() + xlab("") + ylab("") -->
<!-- ``` -->

El ACP lo que busca es un número reducido de dimensión que represente el máximo de variabilidad en las observaciones eliminando la mayor cantidad de ruido posible. El objetivo es crear una serie de variables sintéticas  $Z_{1},\dots, Z_{k}$  con \(k\leq p\), de modo  que posean la misma (o casi) información de las variables originales. 

## Primer componente principal

El primer componente se define como el promedio ponderado de las variables originales \(X_1, \dots, X_p\) 
\begin{equation*} 
Z_{i1} = \phi_{11}x_{i1} + \phi_{21}x_{i2} + \dots + \phi_{p1}x_{ip};\quad \text{con } \sum_{j=1}^{p}\phi_{j1} = 1 
\end{equation*}

El objetivo es que el vector $Z_1$ tenga varianza máxima. Al vector $\phi_1 = (\phi_{11}, \phi_{21},\dots,\phi_{p1})$ se le llama *pesos o cargas* y funciona para promediar cada uno de las columnas de \(X\). Pensemos que $\phi_1$ es la dirección en el espacio característico en $\mathbb{R}^p$ en donde los datos tienen la máxima varianza. 

En nuestro caso defina $X = (X_1,\dots,X_p)_{n\times p}$ la *matriz de diseño* donde cada columna tiene media 0. En este caso queremos resolver el problema 

\begin{equation*}
\hat{\phi}_1=\underset{\Vert\phi_1\Vert_2^2=1}{\mathrm{argmax}} \left\lbrace\dfrac{1}{n}\sum_{i=1}^{n}\left(\sum_{i=1}^p \phi_{j1} X_{ij} \right)^2 \right\rbrace 
\end{equation*}

Note que la restricción de minimización se puede reescribir como $\Vert\phi_1\Vert_2^2= \sum_{j=1}^p \phi_{j1}^2 = 1$

Esta última expresión se podría reescribir de forma matricial como 

\begin{align*}
\hat{\phi}_1 &= \underset{\Vert\phi_1\Vert_2^2=1}{\mathrm{argmax}} \left\{\frac{1}{n} \phi_1^\top X^\top X \phi_1 \right\} \\
\hat{\phi}_1 &= \underset{\Vert\phi_1\Vert_2^2=1}{\mathrm{argmax}} \left\{\phi_1^\top V \phi_1 \right\} \\
\end{align*}

donde $\phi_1 = (\phi_{11}, \phi_{21},\dots,\phi_{p1})$


Ahora s maximimizamos esta expresión usando multiplicadores de lagrange, tenemos que lo siguiente: 

\begin{align*}
L(\phi_1, \lambda) &= \phi_1^\top V \phi_1 - \lambda (\phi_1^\top \phi_1 -1) \\
\frac{\partial L}{\partial \lambda} &= \phi_1^\top \phi_1 -1 \\
\frac{\partial L}{\partial \phi_1} &= 2V\phi_1 - 2\lambda \phi_1 
\end{align*}

Igualando a 0 las condiciones obtenemos que 

\begin{align*}
\phi_1^\top \phi &= 1 \\
V \phi_1 &= \lambda V.
\end{align*}


Entonces el valor óptimo de \(\phi_1\) es el componente principal de $\frac{1}{n}X^\top X = \mathrm{Cov}(X)$ cuyo \(\lambda\) sea más grande si las columnas de $X$ son centradas.

## Segunda componente principal

La segunda componente se escribe de la forma

$$ Z_{2}:= \phi_{12}x_1 + \phi_{22}x_2+\dots+\phi_{p2}x_p.$$

En este caso debemos resolver el mismo problema de máximización que antes. 
 $$\underset{\Vert\phi_2\Vert_2^2=1}{\mathrm{argmax}} \left\lbrace\dfrac{1}{n}\sum_{i=1}^{n}\left(\sum_{i=1}^p \phi_{j2} X_{ij} \right)^2 \right\rbrace$$
 
 
La diferencia es que se quiere que el segundo componente sea ortogonal al primero. Es decir, que $\forall i$, $Z_{i2}\perp Z_1$, entonces 
 $$ Z_{i2}\perp Z_1 \implies \phi_{2} \perp \phi_{1}$$

Esto se logra primero construyendo una matriz nueva de diseño, restando a la matrix $X$ original, el primer componente principal. 

\[
\tilde{X}_2 = X - X\phi_1\phi_1^\top
\]

Luego a esa matriz, se le aplica el procedimiento anterior donde queremos maximizar

\begin{align*}
\hat{\phi}_2 &= \underset{\Vert\phi_2\Vert_2^2=1}{\mathrm{argmax}} \left\{\frac{1}{n} \phi_2^\top \tilde{X}_2^\top \tilde{X}_2 \phi_2 \right\} \\
\hat{\phi}_2 &= \underset{\Vert\phi_2\Vert_2^2=1}{\mathrm{argmax}} \left\{\phi_2^\top \tilde{V}_2 \phi_2 \right\} \\
\end{align*}



Y nuevamente se puede probar que el componente principal corresponde al segundo vector propio de 
$X^\top X = \mathrm{Cov}(X)$


De la misma forma se construye $\phi_3,\phi_4,\dots, \phi_p$.

Notas: 

- **Escalas**: la varianza de las variables depende de las unidades. El problema es que los pesos $\phi_i$ son distintos dependiendo de las escalas. La solución es estandarizar las variables: $\dfrac{X_i-\mu_i}{\hat\sigma_i}$.
- **Unicidad**: los componentes principales son únicos, módulo cambio de signo.


## Circulo de correlaciones 

Se puede construir la correlación de cada variable con respecto a cada componente principal 

\[
cos(\theta_{i,j^\prime}) = \mathrm{Corr}(X_i, \mathrm{PC}_{j^\prime})
\]

El ángulo $\theta_{i,j^\prime}$ significa la lejanía o cercanía de cierta variable con respecto a cada componente principal. 

Además, basados en el el círculo identidad $\cos^2(\theta)+\sin^2(\theta)=1$, el valor de $cos^2(\theta_{i,j^\prime})$ representa la "intensidad" con la cual la variable $X_i$ es representada por el componente principal $\mathrm{PC}_{i^\prime}$.

## Volvamos a nuestro ejemplo 

```{r}
library("factoextra")
library("FactoMineR")
```

```{r}
(PC1 <- prcomp(cbind(x1,x2,x3),center = TRUE))
```
```{r}
str(PC1)
```


```{r}
(PC2 <- princomp(cbind(x1, x2, x3)))
```

```{r}
str(PC2)
```



```{r}
p <- PCA(scale(cbind(x1, x2, x3)))
```

```{r}
p$var$cor
p$var$cos2
```





## ¿Cuántos componentes usar?

```{r}
fviz_screeplot(p, addlabels = F, ylim = c(0, 50)) + xlab("Variables") + ylab("Porcentaje de varianza de Z explicada") + labs(title = "Diagrama")
```
```{r}
qplot(1:3, p$eig[, 3], geom = "point") + xlab("Cantidad de componentes") + ylab("Varianza acumulada") + geom_line() + theme_minimal() + geom_hline(yintercept = 80, color = "red") +
  scale_x_continuous(breaks = 1:10)
```






## Laboratorio

Vamos a usar los datos `decathlon` de `FactomineR` que representa los resultados de varios atletas en pruebas de decathlon en el 2004. 

El objetivo es encontrar si hay patrones entre ciudad y tipos de crimen.

Exploración de datos
Ejecute una exploración de datos


```{r,message=F,echo=F}
data("decathlon")
summary(decathlon)
```


```{r,message=F,echo=F}
acp.decathlon <- PCA(decathlon, quanti.sup = 11:12, quali.sup = 13)
```


```{r,message=F,echo=F}
fviz_screeplot(acp.decathlon, addlabels = F, ylim = c(0, 50)) + xlab("Variables") + ylab("Porcentaje de varianza de Z explicada") + labs(title = "Diagrama")
```


```{r,message=F,echo=F}
pvc <- acp.decathlon$eig[, 3]
```


```{r,message=F,echo=F}
qplot(1:10, pvc, geom = "point") + xlab("Cantidad de componentes") + ylab("Varianza acumulada") + geom_line() + theme_minimal() + geom_hline(yintercept = 80, color = "red") +
  scale_x_continuous(breaks = 1:10)
```


```{r}
plot(acp.decathlon$ind$coord[, 1],
       acp.decathlon$ind$coord[, 2])

plot(acp.decathlon$ind$coord[, 3],
       acp.decathlon$ind$coord[, 4])

```


## Ejercicios 

- Del libro [@James2013b] 
- Capítulo 10:  6, 8


