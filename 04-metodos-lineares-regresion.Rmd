 
# Métodos lineales de regresión

**NOTA: Para los siguientes capítulos nos basaremos en los libros [@HastieElements2009] y [@James2013b].**


## Introducción al Aprendizaje Estadístico.

Supongamos que tenemos \(p\) variables de entrada que provocan una respuesta \(Y\) (variable dependiente) a través de la siguiente relación:

\begin{equation}
Y = f(X_{1},\ldots,X_{p}) + \varepsilon
(\#eq:regresion-general)
\end{equation}
donde \(f\) es desconocida, las variables \(X\)'s son las variables de entrada (covariables o predictores) y \(\varepsilon\) representa un error aditivo a la relación definida por $f$.



Hay dos motivos por los que estimamos \(f\):

1. **Predicción:** Si se estima \(f\) con \(\hat{f}\) entonces
\begin{equation*}
\hat{Y} = \hat{f}(X_{1},\ldots,X_{p}). 
\end{equation*}
asumiendo que el valor medio del error $\epsilon$ es cero. Si tuvieramos valores nuevos de los \(X\)'s entonces podríamos estimar el valor que el corresponde a \(Y\). 

En este caso obtener una estructura óptima o precisa de la función $\hat f$ no es importante, siempre y cuando sea posible obtener buenas predicciones de $Y$. Para entender mejor esta idea se puede definir:

  a. **Error reducible:** Error de \(\hat{f}\) alrededor de \(f\), el cual es propio de la escogencia del modelo. 
  a. **Error irreducible:** Error que escapa a una estimación perfecta de $f$. Puede venir de covariables no consideradas en el problema, fuentes de error que no se pueden cuantificar, etc.

\begin{align*}
\mathbb{E}\left[(\hat{Y}-Y\right)^2] 
&=  \mathbb{E}\left[\left(  f(X_{1},\ldots,X_{p}) + \varepsilon - \hat{f}(X_{1},\ldots,X_{p}) \right)^{2}  \right] \\
&= \underbrace{\left( f(X_{1},\ldots,X_{p})- \hat{f}(X_{1},\ldots,X_{p})  \right) ^{2} }_{\text{Reducible}}
+\underbrace{\mathrm{Var}\left(\varepsilon\right)}_{\text{irreducible}}.
\end{align*}
asumiendo que $f$ y $X$ son conocidas y determinísticas.

2. **Inferencia:**  Entender la relación entre \(X\) y \(Y\), es decir entender cómo $Y$ cambia como función de las covariables. 
En este caso sí nos interesa obtener un estimador preciso e interpretable de la función $f$. Las siguientes preguntas son de interés:
- ¿Cuáles covariables están asociadas con la variable respuesta o dependiente?
- ¿Cuál es la relación entre cada variable predictora y la respuesta?
- ¿La relación entre covariables y variable dependiente es lineal? o ¿la relación es más compleja?

### Formas de estimar $f$

El proceso de estimación de $f$ a través de $\hat f$ se realiza sobre un subconjunto de los datos disponibles. A este conjunto se le llama *datos de entrenamiento*. El resto de los datos se puede utilizar para probar la capacidad predictiva del modelo seleccionado. 

Existen varias clasificaciones de modelos para estimar $f$:

- Modelos paramétricos vs modelos no parámetricos. Los modelos pueden tener parámetros que facilitan el proceso de estimación, pero el número de parámetros debe ser conservador para evitar situaciones de *sobreajuste*. Los modelos no-paramétricos requieren de mucha información para dar un buen ajuste, sea a través de una muestra grande o a través de manipular parámetros generales de suavidad (ancho de banda).
- Modelos predictivos vs modelos interpretativos. Entre más flexible (complejo) sea un modelo, más dificil es su interpretación, por lo tanto más dificil es hacer inferencia. Hay modelos muy flexibles que permiten hacer muy buena predicción, pero fácilmente se puede caer en sobreajuste.
- Modelos supervisados vs no supervisados. ¿La variable $Y$ está disponible en la muestra?
- Modelos de regresión vs modelos de clasificación. ¿La variable $Y$ es continua o es una variable categórica?

### Medidas de bondad de ajuste

En el caso de regresión, la medida más utilizada es el Error Cuadrático Medio (MSE):
\begin{align*}
MSE=\frac 1 n \sum_{i=1}^n(y_i-\hat f(x_i))^2
\end{align*}
calculada sobre la base de entrenamiento del modelo para evaluar la capacidad de ajuste de $\hat f$. Para evaluar la capacidad predictiva del modelo se puede usar el mismo concepto sobre la *base de prueba*. La diferencia entre la magnitud del MSE en los dos conjuntos de datos, puede ser un indicador de sobreajuste. 

Para el caso de un problema de aprendizaje estadístico hay interpretaciones de los componentes de sesgo y varianza:

- *Varianza*: variación de $\hat f$ ante cambios en los datos de entrenamiento. Modelos más flexibles tienen mayor varianza.
- *Sesgo*: error al aproximar la realidad complicada con un modelo más simple.  Modelos más flexibles tienen menor sesgo.

**Estrategia de búsqueda de modelos**: conforme aumenta la flexibilidad de un modelo el sesgo disminuye, y la varianza no aumenta en el mismo ritmo. A partir de un cierto momento la disminución del sesgo no es lo suficientemente fuerte como para contrarrestar el crecimiento en varianza. 

**Conclusión**: un modelo parsimonioso posiblemente garantizará un valor óptimo en MSE.

## Regresión lineal

El caso más sencillo es cuando se asume que la relación es lineal y se describe de la siguiente forma: 

\begin{equation*}
Y = \beta_{0} + \beta_{1}X_{1} + \cdots +  \beta_{p}X_{p} + \varepsilon.
\end{equation*} 
 
 Aquí los valores \(\beta\)'s son constantes a estimar, las variables \(X\)'s son las variables de entrada y  \(\varepsilon\) es el error irreducible cometido por hacer esta aproximación. 
 

Las covariables en un modelo de regresión pueden ser: 

1. Cuantitativas: variables continuas. 
2. Categóricas: variables tipo factor que admiten un número de niveles. Estas variables pueden ser ordinales o nominales, dependiendo si hay un orden natural en la escala de los niveles. Para incorporarla en el modelo de regresión debemos *codificar* la variable:

```{example}
Se tiene la variable \(G\) codificada con Casado (1), Soltero (2), Divorciado (3) y Unión Libre (4). Si queremos incorporar esta variable en una regresión podríamos usar la siguiente codificación: 

\begin{equation*}
X_{j} = \mathbf{1}_{\{G=j+1\}} 
\end{equation*} 

que resulta en la matriz 

\begin{equation*}
\begin{matrix}
X_{1} & X_{2} & X_{3}\\
0 & 0 & 0 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{matrix}
\end{equation*}

Existen otras formas de codificar este tipo de variables, pero esta es una de las más usuales.

```


### Forma matricial

Podemos escribir el modelo de regresión en forma matricial: 

\begin{equation*}
\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{equation*}

donde 

\begin{multline*}
\boldsymbol{Y} = 
\begin{pmatrix}
Y_{1} \\
\vdots \\
Y_{n}
\end{pmatrix}_{n\times 1} 
\quad 
\boldsymbol{X} = 
\begin{pmatrix}
1 & X_{1,1} & \cdots & X_{p,1} \\
\vdots & \vdots & \cdots & \vdots\\
1 & X_{1,n}& \cdots & X_{p,n}
\end{pmatrix}_{n\times (p+1)}
\\
\boldsymbol{\varepsilon} = 
\begin{pmatrix}
\varepsilon_{1} \\
\vdots \\
\varepsilon_{n}
\end{pmatrix}_{n\times 1} 
\quad 
\boldsymbol{\beta} = 
\begin{pmatrix}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{p}
\end{pmatrix}_{(p+1)\times 1} 
\end{multline*}

Suponemos que \(\mathbb{E}\left[\varepsilon_{i}\right] = 0\) y \(\mathrm{Var}\left(\varepsilon_{i}\right) = \sigma^{2}\).

La forma de resolver este problema es  por minimos cuadrados.  Es decir, buscamos el \(\hat{\beta}\) que cumpla lo siguiente:


\begin{align}
\hat{\beta} &= 
 \operatorname{argmin}_\beta (\boldsymbol{Y} - \boldsymbol{X} \boldsymbol{\beta})^{\top} (\boldsymbol{Y} - \boldsymbol{X} \boldsymbol{\beta})\\
 &=  \operatorname{argmin}_\beta \sum_{i=1}^n \left( Y_{i} -\beta_{0} - \sum_{j=1}^p X_{j,i} \beta_{j} \right)^2 
 (\#eq:minimos-cuadrados)
 \end{align}


![Tomado de https://www.wikiwand.com/en/Ordinary_least_squares](manual_figures/ols.png)
Por lo tanto buscaríamos minimizar la suma de *residuos* al cuadrado.

Suponga que \(\gamma\) es un vector cualquiera en \(\mathbb{R}^{p+1}\)  y defina \(V := \{\boldsymbol{X}\boldsymbol{\gamma}, \gamma \in \mathbb{R}^{p+1}\}\), es decir el espacio lineal generado por las columnas (covariables) de $\boldsymbol{X}$. Buscamos entonces un vector $\beta$ que cumpla:

\begin{align*}
\boldsymbol{X}\boldsymbol{\beta}
 &= \operatorname{Proy}_{V} \boldsymbol{Y}
\end{align*}

Entonces dado que $\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \perp V$, es decir $\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta} \perp \boldsymbol{X}\boldsymbol{\gamma}, \forall \boldsymbol{\gamma} \in \mathbb{R}^{p+1}$ entonces:

\begin{align*}
 \boldsymbol{X}\boldsymbol{\gamma} \cdot \left(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta}\right)  &=  0 \\
 \boldsymbol{\gamma}^{\top}\boldsymbol{X}^{\top}(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta}) &=  0 \\
 \boldsymbol{\gamma}^{\top}\boldsymbol{X}^{\top}\boldsymbol{Y} &= \boldsymbol{\gamma}^{\top} \boldsymbol{X}^{\top} \boldsymbol{X}\boldsymbol{\beta}  \\
  \boldsymbol{X}^{\top}\boldsymbol{Y} &=  \boldsymbol{X}^{\top} \boldsymbol{X}\boldsymbol{\beta}  \\
  \boldsymbol{\beta}  &=  (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top}\boldsymbol{Y} 
\end{align*}




Donde se asume que \(\boldsymbol{X}^{\top} \boldsymbol{X}\) debe ser invertible. Si no es así, se puede construir su inversa generalizada pero no garantiza la unicidad de los \(\beta\)'s. Es decir, puede existir \(\hat{\beta} \neq \tilde{\beta}\) tal que \(\boldsymbol{X}\boldsymbol{\hat{\beta}} = \boldsymbol{X}\boldsymbol{\tilde{\beta}} \). A $\hat \beta$ se le llama estimador por mínimos cuadrados de $\beta$.

En el caso de predicción tenemos que 

\begin{align*}
\hat{Y} &=  X\hat \beta \\
&= \boldsymbol{X}(\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top}\boldsymbol{Y} \\
&=  H \boldsymbol{Y} 
\end{align*}

Donde \(H\) es la matriz "techo" o  "hat". La matriz $H$ es la matriz de proyección de Y al espacio de las columnas de \(X\).

```{exercise}
Suponga que tenemos la regresión simple

\begin{equation*}
Y = \beta_{0} + \beta_{1}X_{1}+\varepsilon.
\end{equation*}


Verifique que los estimadores de mínimos cuadrados de \(\beta_{0}\) y \(\beta_{1}\) son: 

\begin{align*}
\hat{\beta}_{1}&= \frac{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)\left(Y_{i}-\overline{Y}\right)}{\sum_{i=1}^{n}\left(X_{i}-\overline{x}\right)^{2}} \\ 
\hat{\beta}_{0}&= \bar{Y}-\widehat{\beta}_{1} \bar{X}
\end{align*}

usando los siguiente métodos:

1. El método de proyecciones. 
2. Aplicando el criterio de mínimos cuadrados. Ecuación \@ref(eq:minimos-cuadrados).

```

### Laboratorio

Usemos la base `mtcars` para los siguientes ejemplos. Toda la información de esta base se encuentra en `?mtcars`. 


```{r}
mtcars <- within(mtcars, {
  vs <- factor(vs, labels = c("V-Shape", "Straight-Line"))
  am <- factor(am, labels = c("automatic", "manual"))
  cyl  <- factor(cyl)
  gear <- factor(gear)
  carb <- factor(carb)
})

head(mtcars)
summary(mtcars)
```



Observemos las relaciones generales de las variables de esta base de datos 

```{r}
ggplot(mtcars) + 
  geom_point(aes(wt, mpg)) + 
  theme_minimal()
```

El objetivo es tratar la eficiencia del automóvil `mpg` con respecto a su peso `wt`. 


Usaremos una regresión lineal para encontrar los coeficientes. 

Primero hay que construir la matriz de diseño

```{r}
X <-  mtcars$wt
head(X)
Y <- mtcars$mpg
head(Y)

(beta1 <-  solve(t(X) %*% X) %*% t(X) %*% Y)

dfreg <- data.frame(x = X, yreg = X %*% beta1) %>% arrange(x)
```

```{r}
ggplot(data = data.frame(x = X, y = Y)) +
  geom_point(aes(x, y)) +
  geom_line(data = dfreg, aes(x, yreg), color = "red") +
  theme_minimal()
```

en donde podemos concluir que la relación lineal no modela de manera apropiada la relación observada en los datos. Por lo tanto es necesario incluir el intercepto $\beta_0$ al modelo lineal:

```{r}
X <-  cbind(1, mtcars$wt)
head(X)
Y <- mtcars$mpg
head(Y)

(beta01 <-  solve(t(X) %*% X) %*% t(X) %*% Y)

dfreg <- data.frame(x = X, yreg = X %*% beta01) %>% arrange(x.2)
```
```{r}
ggplot(data = data.frame(x0 = X[, 1], x1 = X[, 2], y = Y)) +
  geom_point(aes(x1, y)) +
  geom_line(data = dfreg, aes(x.2, yreg), color = "red") +
  theme_minimal()
```

El mismo resultado se puede obtener a través del comando \verb|lm|: 

```{r}

lm(mpg ~ -1 + wt, data = mtcars)

lm(mpg ~ wt, data = mtcars)

```

Suponga que queremos trabajar con la variable categorica `cyl` (Número de cilindros) como única covariable.  Lo que se debe hacer es codificar la variable categórica: 

```{r}
X <- model.matrix(mpg ~ cyl, data = mtcars)

head(X)

(betas <-  solve(t(X) %*% X) %*% t(X) %*% Y)

(cylreg <- lm(mpg ~ cyl, data = mtcars))

(betaslm <- coefficients(cylreg))
```

```{r}
# Efecto cyl4: 
# cyl4 = 1, cyl6 = 0, cyl8 = 0

betaslm[1]

# Efecto cyl6:
# cyl4 = 1, cyl6 = 1, cyl8 = 0

betaslm[1] + betaslm[2]


# Efecto cyl8: 
# cyl4 = 1, cyl6 = 0, cyl8 = 1

betaslm[1] + betaslm[3]
```



## Propiedades estadísticas 

Hasta ahora se han hecho pocos supuestos acerca de la distribución de los datos. Si asumimos que las observaciones $Y_i$ son no correlacionadas y que tienen varianza constante $\sigma^2$ y además las covariables son fijas (no aleatorias), entonces:

\begin{align*}
E[\hat \beta]&=(\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top}E[\boldsymbol{Y}] \\ 
&=(\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top}\boldsymbol{X}\beta\\
&=\beta \\
\text{Var}[\hat \beta] &= (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top}\text{Var}[\boldsymbol{Y}] ((\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top})^{\top} \\
& = \sigma^2 (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1}
\end{align*}

Note que \(\sigma^{2}\) puede ser estimado a través de:
 
 \begin{align*}
 \hat{\sigma}^{2} 
 &=  \frac{1}{n-p-1} \sum_{i=1}^{n} \left( Y_{i} - \hat{Y}_{i}\right)^{2} \\
 &= \frac{1}{n-p-1}\left\Vert Y - X\hat{\beta} \right\Vert^{2} \\
 &=   \frac{1}{n-p-1} \left\Vert Y-\operatorname{Proy}_{V}Y \right\Vert^{2} 
 \end{align*}
 
 Otra forma de verlo es 
 \begin{align*}
Y-\operatorname{Proy}_{V}Y  
&= X\beta + \varepsilon -  \operatorname{Proy}_{V}( X\beta + \varepsilon) \\
&= X\beta - \operatorname{Proy}_{V}( \underbrace{X\beta}_{\in V}) + \varepsilon - \underbrace{\operatorname{Proy}_{V}( \varepsilon)}_{=0} \\
&= X\beta -X\beta + \varepsilon \\
&=  \operatorname{Proy}_{V^{\top}}( \varepsilon)
 \end{align*}

\begin{equation*}
\hat{\sigma}^{2} 
= \frac{1}{\operatorname{dim}(V^{\top})}\left\Vert \operatorname{Proy}_{V^{\top}}\varepsilon\right\Vert \\
\end{equation*}
 
Cumple con la propiedad que \(\mathbb{E}\left[\hat{\sigma}^{2}\right] = \sigma^{2}\) (estimador insesgado).

Para poder hacer inferencia sobre $\beta$ y $\sigma^2$ se puede asumir además que los errores son gaussianos:

\begin{equation*}
\varepsilon\sim \mathcal{N}\left(0,\sigma^{2}I\right).
\end{equation*} 

y de esta forma se obtiene:

\begin{equation*}
Y = X\beta + \varepsilon \sim \mathcal{N}\left(X\beta,\sigma^{2}I\right)
\end{equation*} 

Y además: 

\begin{align*}
\hat{\beta} \sim  \mathcal{N}\left(\beta,\sigma^2 (X^{\top}X)^{-1}\right) 
\end{align*}

Por otro lado se puede comprobar que: 
$$(n-p-1)\hat{\sigma}^{2} \sim \sigma^{2} \chi^{2}_{n-p-1}.$$
y además se puede comprobar que $\hat \beta$ y $\hat \sigma^2$ son independientes. 

```{exercise}
Encuentre la varianza para \(\hat \beta_{0}\) y \(\hat \beta_{1}\) para el caso de la regresión simple. 
```

### Prueba \(t\)
 
La significancia de los parámetros $\beta_j$ se puede verificar a través de la siguiente prueba de hipótesis:

 \begin{equation*}
 H_{0}: \beta_{j} = 0 \quad \text{ vs } \quad H_{1}:\beta_{j}\neq 0.
 \end{equation*}
 
 En donde el estadístico de prueba es: 
 
 \begin{equation*}
 z_{j} = \frac{\hat{\beta}_{j}}{\hat{\sigma} \sqrt{v_{j}}} 
 \end{equation*}
 
 donde \(v_{j}\) es el \(j\)-esimo elemento de la diagonal de \((X^{\top}X)^{-1}\).
 
 Bajo \(H_{0}\):  \(z_{j} \sim t_{n-p-1}\) y se rechaza \(H_{0}\) al nivel $\alpha$ si: 
 
 \begin{equation*}
 \left\vert z_{j} \right\vert > t_{n-p-1, 1-\frac{\alpha}{2}} 
 \end{equation*}
 
### Prueba \(F\)

Si uno busca medir la significancia de todos los parámetros $\beta_j$ de forma simultánea, excepto el intercepto. En este caso podemos definir la siguiente hipótesis nula:
 \begin{equation*}
 H_{0}: \beta_{1} = \cdots =\beta_{p} = 0 \quad 
 \text{  vs   }\quad H_{1}: \text{ al menos un \(\beta\) no es cero}.
 \end{equation*}
 
 
Lo cual es equivalente a comparar el modelo  nulo \(Y=\beta_{0}+\varepsilon\) contra el modelo completo \(Y=\beta_{0}+ \beta_{1}X_{1} + \cdots + \beta_{p}X_{p} + \varepsilon\). 
 
 Defina la suma total de cuadrados ($TSS$) y la suma de residuos al cuadrado ($RSS$) como: 
 \begin{align*}
 TSS &= \sum_{i=1}^{n} \left( Y_{i} -\overline{Y} \right)^{2} \\
 RSS &= \sum_{i=1}^{n} \left( Y_{i} -\hat{Y_i} \right)^{2} \\
 \end{align*}
 
 Entonces el estadístico de prueba es:
 
 \begin{equation*}
 F = \frac{\frac{TSS-RSS}{p}}{\frac{RSS}{n-p-1}} \stackrel{H_0}{\sim} \frac{\chi^{2}_{p}}{\chi^{2}_{n-p-1}}.
 \end{equation*}
 
 y rechazaríamos \(H_{0}\) al nivel $\alpha$ si: 
 
 \begin{equation*}
 F > F_{p, n-p-1, 1-\alpha}.
 \end{equation*}
 
 Si por otro lado queremos probar que un conjunto de $q$ covariables son no-significativas entonces probamos (sin pérdida de generalidad):
\begin{align*}
H_0: \beta_{p-q+1}=\beta_{p-q+2}=\cdots=\beta_p=0
\end{align*}

a través de la comparación de un modelo completo y uno reducido:
\begin{align*}
Y&=\beta_{0}+ \beta_{1}X_{1} + \cdots + \beta_{p}X_{p} + \varepsilon \qquad \text{Modelo completo} \\
Y&=\beta_{0}+ \beta_{1}X_{1} + \cdots + \beta_{p-q}X_{p-q} + \varepsilon \qquad \text{Modelo reducido}
\end{align*}

usando el estadístico de prueba:

 \begin{equation*}
 F = \frac{\frac{RSS_0-RSS}{q}}{\frac{RSS}{n-p-1}} \stackrel{H_0}{\sim} \frac{\chi^{2}_{q}}{\chi^{2}_{n-p-1}}.
 \end{equation*}

donde $RSS_0$ es la suma de residuos al cuadrado del modelo reducido. En este caso se rechazaría $H_0$ al nivel $\alpha$ si $F>F_{q, n-p-1, 1-\alpha}$. 
 
###  Laboratorio


Siguiendo con nuestro ejemplo, vamos a explorar un poco más la función `lm`. 
```{r}
modelo_wt <- lm(mpg ~ wt, data = mtcars)
summary(modelo_wt)

modelo_wt_cyl <- lm(mpg ~ wt + cyl, data = mtcars)
summary(modelo_wt_cyl)

anova(modelo_wt,modelo_wt_cyl)

modelo_nulo <- lm(mpg ~ 1, data = mtcars)
summary(modelo_nulo)

anova(modelo_nulo,modelo_wt_cyl)

fit <- lm(mpg ~ . , data = mtcars)
summary(fit)

```

## Medida de bondad de ajuste

A través de la prueba $F$ uno puede concluir si un modelo es significativo o no bajo un cierto nivel de confianza, o bien puede comparar si un modelo reducido es más significativo que uno completo, pero no nos da herramientas para decidir si un modelo es mejor que otro.

Hay varias medidas para comparar modelos (la veremos con más detalle en otro capítulo):

- Error estándar residual (\(\sigma\))
- $R^{2}$ y $R^{2}$ ajustado
- \(C_{p}\) de Mallows
- Akaike Information Criterion (AIC)
- Bayesian Information Criterion (BIC)

Los índices \(C_{p}\) de Mallows, AIC y BIC los veremos después. 

Error estándar residual
: Se define como 

\begin{align*}
\mathrm{RSE} 
&=  \sqrt{\hat{\sigma^{2}}}\\
&= \sqrt{\frac{1}{n-p-1} \sum_{i=1}^{n} \left( Y_{i} - \hat{Y}_{i}\right)^{2}} \\
&= \sqrt{\frac{\mathrm{RSS}}{n-p-1}}
\end{align*}

Entre más pequeño mejor, pero **depende de las unidades de \(Y\)**.

Estadístico \(R^{2}\) 
: 
\begin{equation*}
R^{2} = \frac{\mathrm{TSS}-\mathrm{RSS}}{\mathrm{TSS}} = 1-\frac{\mathrm{RSS}}{\mathrm{TSS}}
\end{equation*}

- **RSS:** Varianza sin explicar por el modelo __completo__.
- **TSS:** Varianza sin explicar por el modelo __nulo__.

Interpretación: proporción de variabilidad en $Y$ que es explicada a través de las covariables en $X$. Ya que $TSS-RSS$ representa la variabilidad explicada a través del modelo de regresión.

Limitación: puede tener un valor alto bajo un número grande de covariables, ya que $RSS$ tiende a ser bajo conforme aumenta la complejidad del modelo (sobreajuste).


Estadístico \(R^{2}\) ajustado
: 
\begin{equation*}
R^{2}_{adj} = 1-\frac{\frac{\mathrm{RSS}}{n-p-1}}{\frac{\mathrm{TSS}}{n-1}}
\end{equation*}

 
### Laboratorio

```{r}
# Número de datos
n <- 1000
# Número de variables
p <- 2

x1 <- rnorm(1000)
x2 <- runif(1000)
y <- 1 + x1 + x2 + rnorm(1000, sd = 0.5)

fit <- lm(y ~ x1 + x2)

```


#### $R^2$
```{r}
(TSS <- sum((y - mean(y)) ^ 2))
(RSS <- sum((y - fitted(fit)) ^ 2))

1 - RSS / TSS

```

Otra forma de entender el $R^2$ es notando que 

```{r}
cor(y, fitted(fit))^2
```



#### $R^2$ ajustado
```{r}

(TSS_adj <- TSS / (n - 1))
(RSS_adj <- RSS / (n - p - 1))


1 - RSS_adj / TSS_adj
```


#### `summary`
```{r}
summary(fit)
```



 
## Predicción 

Hay dos tipos de errores que se deben considerar en regresiones lineales: 

1. **Error Reducible:** Recuerde que \(\hat{Y} = X\hat{\beta} \) es el estimador de la función \(f(X)=X\beta = \beta_{0} + \beta_{1}X_{1}+\cdots+\beta_{p}X_{p}\). 

Por lo tanto su error (reducible) es: 

\begin{equation*}
\left(  f(X) - \hat{Y}\right) ^{2}. 
\end{equation*}

Para un conjunto de datos \(X_{0}\), tenemos que  

\begin{align*}
 & \hat{\beta} \sim  \mathcal{N}\left(\beta, \sigma^{2}\left( (X_{0}^{\top}X_{0})^{-1} \right)\right) \\
 \implies & \hat{Y} = X_{0}\hat{\beta} \sim \mathcal{N}\left((X_{0}\beta,\sigma^{2}X_{0}^{\top}((X_{0}^{\top}X_{0})^{-1}X_{0})) \right)
\end{align*}

Por lo tanto un **intervalo de confianza** al \(1-\alpha\) para \(X_0\beta\) es 

\begin{equation*}
X_{0}\hat \beta \pm z_{1-\frac{\alpha}{2}} \hat{\sigma} \sqrt{X_{0}^{\top}(X_{0}^{\top}X_{0})^{-1}X_{0}}.
\end{equation*}

2. **Error irreducible:** Aún conociendo perfectamente los \(\beta\)'s, existe  el error desconocido \(\varepsilon\sim \mathcal{N}\left(0,\sigma^{2}\right)\) del modelo 

\begin{equation*}
Y = X\beta + \varepsilon.
\end{equation*} 

Entonces la varianza total de la predicción sería 

\begin{equation*}
\sigma^{2} +  \sigma^{2}X_{0}^{\top}((X_{0}^{\top}X_{0})^{-1}X_{0}) 
\end{equation*}


Entonces un **intervalo de predicción** al \(1-\alpha\) debe tomar en cuenta ese error  y por lo tanto 

\begin{equation*}
X_{0}\beta \pm z_{1-\frac{\alpha}{2}} \hat{\sigma} \sqrt{1+X_{0}^{\top}(X_{0}^{\top}X_{0})^{-1}X_{0}}.
\end{equation*}


### Laboratorio 

```{r}
lm.r <- lm(mpg ~ wt, data = mtcars)

range(mtcars$wt)
(datos_nuevos <-  data.frame(wt = c(2.5,3,3.5)))
predict(object =  lm.r,
        newdata = datos_nuevos,
        interval = "confidence")

predict(object =  lm.r,
        newdata = datos_nuevos ,
        interval = "prediction")
```




####  Ajuste de la regresión sin intervalos de confianza

```{r}
p <- ggplot(mtcars, aes(x = wt, y = mpg)) 
p <- p + geom_point(size = 2)       # Use circulos de tamaño 2
p <- p + geom_smooth(method = lm,   # Agregar la línea de regresión 
              se = FALSE,           # NO incluir el intervalo de confianza   
              size = 1,
              col = "red")          # Línea de color rojo 
p <- p + theme_bw()                 # Tema de fondo blanco
p <- p + theme(axis.text = element_text(size = 20),  # Aumentar el tamaño 
               axis.title = element_text(size = 20)) # de letra en los ejes

# Dibujar el gráfico
p   

# # Guardar el gráfico en un archivo pdf
# ggsave(filename = 'linear_reg_sin_IC.pdf') # 
```

#### Ajuste de la regresión con intervalos de confianza

```{r}
p <- ggplot(mtcars, aes(x = wt, y = mpg)) 
p <- p + geom_point(size = 2)       # Use circulos de tamaño 2
p <- p + geom_smooth(method = lm,   # Agregar la línea de regresión 
              se = TRUE,            # Incluir el intervalo de confianza   
              size = 1,
              col = "red")          # Línea de color rojo 
p <- p + theme_bw()                 # Tema de fondo blanco
p <- p + theme(axis.text = element_text(size = 20),  # Aumentar el tamaño 
               axis.title = element_text(size = 20)) # de letra en los ejes

# Dibujar el gráfico
p   

# # Guardar el gráfico en un archivo pdf
# ggsave(filename = 'linear_reg_con_IC.pdf') # 
```

#### Ajuste de la regresión con intervalos de confianza y predicción

```{r}
# Agregamos a mtcars el intervalo de predicción para cada dato
mtcars.pred <-
  data.frame(mtcars, predict(lm.r, interval = "prediction"))

p <- ggplot(mtcars.pred, aes(x = wt, y = mpg))
# Use circulos de tamaño 2
p <- p + geom_point(size = 2)
# Agregue una banda de tamaño [lwr, upr] para cada punto
# y llamela 'predicción'
p <- p + geom_ribbon(aes(ymin = lwr,
                         ymax = upr,
                         fill = 'predicción'),
                     alpha = 0.3)
# Agregue el intervalo de confianza usual
# y llame a ese intervalo 'confianza'
p <- p + geom_smooth(
  method = lm,
  aes(fill = 'confianza'),
  size = 1,
  col = 'red'
)
# Para agregar bien las leyendas
p <- p + scale_fill_manual('Intervalos',
                           values = c('green', 'yellow'))
p <- p + theme_bw()
p <- p + theme(axis.text = element_text(size = 20),
               axis.title = element_text(size = 20))

# Dibujar el gráfico
p   

# # Guardar el gráfico en un archivo pdf
# ggsave(filename = 'linear_reg_con_IC_IP.pdf') # 
```


Repitamos el mismo ejercicio anterior pero con un caso más sencillo. 

```{r}
n <- 1000

X <- runif(n, 0, 10)
Y <- 10 + sin(5*X) + X + rnorm(1000, 0, 1)
toyex.initial <- data.frame(X, Y) %>% arrange(X)

plot(toyex.initial)

```


```{r}
lm.toyex.initial <- lm(Y ~ X, data = toyex.initial)

summary(lm.toyex.initial)

toyex.pred.initial <- data.frame(toyex.initial, predict(lm.toyex.initial, interval = "prediction"))
```

Ahora, quisiera generar muchas muestras del mismo experimento

```{r}
toyex.pred <- NULL

for (i in 1:10) {
  X <- runif(n,0,10)
  Y <- 10 + sin(5*X) + X + rnorm(1000, 0, 1)
  toyexi <- data.frame(im = i, X, Y)
  toyexi <- toyexi %>% arrange(X)
  toyex.pred <-
    bind_rows(toyex.pred, data.frame(toyexi, predict(lm.toyex.initial, interval = "prediction")))
}


for(i in 1:10) {
  toyex.pred$fit <-
    fitted(lm(formula = Y ~ X,
              data = toyex.pred[toyex.pred$im == i, ]))
}

toyex.pred$im <- as.factor(toyex.pred$im)
```


```{r, eval=FALSE}
library(gganimate)

ggplot(data = toyex.pred,
       aes(x = X, y = Y)) +
  geom_point(size = 1) +
    geom_smooth(
    data = toyex.initial, 
    method = lm,
    mapping = aes(fill = "confianza"),
    size = 1,
    col = 'red'
  ) +
  geom_ribbon(
    data = toyex.pred.initial,
    mapping = aes(
      x = X,
      ymin = lwr,
      ymax = upr,
      fill = 'predicción',
    ),
    alpha = 0.3
  ) +
  labs(title = paste0("Muestra #: {closest_state}")) +
  scale_fill_manual('Intervalos',
                    values = c('green', 'yellow')) +
  theme_bw() +
  theme(axis.text = element_text(size = 20),
        axis.title = element_text(size = 20)) +
  transition_states(im)

  

```

## Interacciones 

Suponga un modelo lineal con dos covariables:

\begin{equation*}
Y = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \varepsilon
\end{equation*}

Aumentemos en 1 unidad \(X_{1}\) y rescribamos el modelo original 

\begin{align*}
Y &=  \beta_{0} + \beta_{1} (X_{1}+1) + \beta_{2} X_{2} + \varepsilon \\
Y &=  (\beta_{0} + \beta_{1}) + \beta_{1} X_{1} + \beta_{2} X_{2} + \varepsilon \\
Y &=  \tilde{\beta_{0}} + \beta_{1} X_{1} + \beta_{2} X_{2} + \varepsilon \\
\end{align*}

Es decir, el modelo original sigue siendo teniendo la misma estructura aunque hayamos cambiado el \(X_1\). Este fenómeno ocurre siempre bajo transformaciones lineales de las variables. 


Ahora suponga que tenemos el siguiente modelo:
\begin{align*}
Y =  \beta_{0} + \beta_{1} X_{1} X_{2} +\varepsilon \\
\end{align*}

y aumentamos en 1 el \(X_1\):

\begin{align*}
Y &=  \beta_{0} + \beta_{1} (X_{1}+1) X_{2} +\varepsilon \\
Y &=  \beta_{0} + \beta_{1}X_{2} +  \beta_{1} X_{1} X_{2} +\varepsilon \\
\end{align*}

Note que en este caso no se logra mantener el mismo tipo de estructura. Una forma de arreglar el problema es incluir las *interacciones* junto con todos sus *efectos principales*. 

\begin{equation*}
Y =  \beta_{0} + \beta_{1}X_{1} + \beta_{2} X_{2} +  \beta_{3} X_{1} X_{2} +\varepsilon \\
\end{equation*}

Este modelo se le conoce como modelo lineal con interacciones (caso de 2 covariables). Este modelo considera la posible interacción entre las covariables $X_1$ y $X_2$ que permiten cambiar tanto el intercepto como las pendientes de los efectos principales.

**Principio de jerarquía**. Con el fin de mantener la estructura del modelo lineal, siempre es necesario incluir los efectos principales cuando se determina que una interacción entre ellos es significativa.


```{exercise}
Compruebe que para el caso anterior, si aumenta en una unidad \(X_{1}\), el modelo preserva su estructura. 
```

### Laboratorio


Generamos una base de datos nueva con solamente `wt` centrado

```{r}
# La función across y where solo funciona solo para dplyr 1.0
# Si tienen otra versión, pueden usar mutate_if

mtcars_centered <- mtcars %>%
  mutate(across("wt", scale, scale = FALSE, center = TRUE))

# Si no se tiene dplyr 1.0

mtcars_centered <- mtcars %>%
  mutate_at("wt", scale, scale = FALSE, center = TRUE)


```


Compare lo que ocurre con los coeficientes de la base original y la nueva base. 

```{r}
summary(lm(mpg ~ wt + disp, data = mtcars))
summary(lm(mpg ~ wt + disp, data = mtcars_centered))
```


Supongamos que formamos un modelo con solo la interacción y no incluimos los efectos directos. 

```{r}
summary(lm(mpg ~ wt * disp - wt - disp, data = mtcars))
summary(lm(mpg ~ wt * disp - wt - disp, data = mtcars_centered))
```


El modelo correcto sería el siguiente:

```{r}
summary(lm(mpg ~ wt + disp + wt * disp, data = mtcars))
summary(lm(mpg ~ wt + disp + wt * disp, data = mtcars_centered))
```


:::{.exercise data-latex=""}

Repita los comandos anteriores con la siguiente base de datos y explique los resultados. 
```{r}
mtcars_scaled <- mtcars %>%
  mutate(across(c("wt", "disp"), scale, scale = TRUE, center = TRUE))
```

:::


<!-- Y aumentamos -->


<!--  aumenta en 1 unidad   y denotamos  -->

<!-- \begin{equation*} -->
<!-- Y_{+1} = \beta_{0} + \beta_{1} (X_{1}+1) + \beta_{2} X_{2} + \varepsilon -->
<!-- \end{equation*} -->

<!-- vemos que  -->
<!-- \begin{align*} -->
<!-- Y_{+1} -Y &=  \beta_{0} + \beta_{1} (X_{1}+1) + \beta_{2} X_{2} + \varepsilon \\ -->
<!-- & -(\beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \varepsilon) \\ -->
<!-- &= \beta_{1}. -->
<!-- \end{align*} -->

<!-- Entonces \(\beta_{1}\) es la __razon de cambio__ discreta de aumentar 1 unidad en \(X_{1}\) con respecto a \(Y\).  -->


<!-- En otras palabras,  -->

<!-- Ahora suponga que tenemos los modelos: -->

<!-- \begin{align*} -->
<!-- Y &=  \beta_{0} + \tilde{\beta_{1}} X_{1} X_{2} +\varepsilon\\ -->
<!-- Y_{+1} &=  \beta_{0} + \tilde{\beta_{1}} (X_{1}+1) X_{2}+\varepsilon \\ -->
<!-- \end{align*} -->

<!-- y hacemos el mismo cálculo que antes:  -->

<!-- \begin{align*} -->
<!-- Y_{+1} -Y &=  \beta_{0} + \tilde{\beta_{1}} (X_{1}+1) X_{2}+\varepsilon \\ -->
<!-- & -(\beta_{0} + \tilde{\beta_{1}} X_{1} X_{2} +\varepsilon) \\ -->
<!-- &=  \tilde{\beta_{1}} X_{2} -->
<!-- \end{align*} -->

<!-- Es decir esa razón de cambio depende  -->






## Supuestos 

El modelo lineal tiene los siguientes supuestos:

Linealidad
: En la forma lineal de la relación variable dependiente-covariables.

Errores centrados
: $\mathbb{E}(\varepsilon_i) = 0$.

Homocedasticidad
: $\text{Var}(\varepsilon_t) = \mathbb{E}(\varepsilon_t - \mathbb{E} \varepsilon_t)^2 = \mathbb{E} \varepsilon_t^2 = \sigma^2$ para todo $t$. Es decir, la varianza del modelo (**error irreducible**) no depende de las variables independientes u otro factor. 

Normalidad de los residuos
: $\varepsilon \sim N(0, \sigma^2 )$.

Independencia de los errores
: $\text{Cov}(\varepsilon_t,\varepsilon_s ) = \mathbb{E} (\varepsilon_t - \mathbb{E} \varepsilon_t) (\varepsilon_s - \mathbb{E} \varepsilon_s) = \mathbb{E} \varepsilon_t \varepsilon_s = 0$ para todo $t,s$ con $t\neq s$: si para una observación dada existe un error, este no debe depender del error de otra observación.

Si este supuesto no se cumple puede provocar que los errores estándar en intervalos de confianza y predicción sean subestimados. Es decir que un intervalo del 95\% tendrá un margen de error menor y se rechazaría más fácilmente la hipotesis nula de las pruebas $t$ y $F$.

Multicolinealidad 
: Se asume que la matriz $X^TX$ es invertible, es decir $X$ es una matriz de rango completo. Para esto cada una las covariables no debe ser linealmente dependientes, es decir $X^TX$ de debe acercarse a ser a una matriz singular con determinante cercano a 0. Es decir que cada variable explica aproximadamente "un aspecto o característica" del modelo. Sin embargo puede pasar que varias variables expliquen la misma característica y el modelo se vuelve __inestable__ por decidir entre las dos variables. Por ejemplo: la temperatura en grados centigrados y farenheit. 

Esto generaría que \(\mathrm{Var}\left(\hat{\beta}\right)\) sea alto ya que 
\begin{equation*}
\text{Var}(\hat{\beta}) =  \sigma^2(X^{\top}X)^{-1}
\end{equation*}


Más observaciones que predictores
: En caso contrario existen formas alternativas de definir el problema de regresión. (Volveremos a esto cuando veamos selección de modelos)


### Chequeos básicos de las hipótesis de regresión lineal 

#### Linealidad, Errores con esperanza nula, Homocedasticidad

Estos supuestos se puede constantar a partir de un gráfico  de residuos ya que en el caso ideal  \(e_{i} = \hat{Y}_{i}- Y_{i}  \perp \hat{Y}_{i}\). Entonces si este gráfico presenta patrones, quiere indicar que la regresión, no es lineal, que los errores no tienen esperanza nula y que la varianza no es constante. 

Se pueden aplicar transformaciones para resolver estos problemas. Normalmente se usan transformaciones como raiz cuadrada o logaritmos. 


:::{.example data-latex=""}


**Caso ideal**
```{r}
x <- rnorm(1000)
y <- x + rnorm(1000, sd = 0.5)

fit <- lm(y ~ x)
plot(x, y)
abline(a = coef(fit)[1],
       b = coef(fit)[2],
       col = "red")
```


```{r grafico-residuos-lineal, fig.cap='Gráfico de residuos caso lineal'}
plot(fitted(fit), residuals(fit))
abline(h = 0, col = "red")
```



**Caso no-lineal**
```{r}
x <- exp(rnorm(1000))
y <- log(x) + rnorm(1000, sd = 0.5)

fit <- lm(y ~ x)
plot(x, y)
abline(a = coef(fit)[1],
       b = coef(fit)[2],
       col = "red")
```


```{r grafico-residuos-no-lineal, fig.cap='Gráfico de residuos caso no-lineal'}
plot(fitted(fit), residuals(fit))
abline(h = 0, col = "red")
```


**Caso no-lineal transformado**

```{r}
xt <- log(x)


fit <- lm(y ~ xt)
plot(xt, y)
abline(a = coef(fit)[1],
       b = coef(fit)[2],
       col = "red")

plot(fitted(fit), residuals(fit));abline(h=0, col="red")

```
:::


#### Independencia de los errores

En este caso defina $\rho(k) = \text{Cov}(\varepsilon_i,\varepsilon_{i+k} )$. Si los residuos son independientes, entonces debe ocurrir que 

\begin{equation*}
\rho(k) = \begin{cases}
1 & k=0\\
0 & k\neq 0.
\end{cases}  
\end{equation*}

Se calcula la función de autocorrelación empírica y se grafica para analizar su comportamiento 

**Caso ideal**
```{r}
x <- rnorm(1000)
y <- 1 + x + rnorm(1000, sd = 1)
```


```{r}
fit <- lm(y ~ x)
plot(x, y)
abline(a = coef(fit)[1],
       b = coef(fit)[2],
       col = "red")
```


```{r}
summary(fit)
acf(residuals(fit))
```

**Caso errores auto-correlacionados**

```{r}
x <- rnorm(1000)
y <- 1 + x + diffinv(rnorm(999, sd = 1), lag = 1)
```


```{r}
fit <- lm(y ~ x)
plot(x, y)
abline(a = coef(fit)[1],
       b = coef(fit)[2],
       col = "red")
```


```{r}
summary(fit)
acf(residuals(fit))
```


#### Normalidad de los errores 

Este hipótesis es crucial para hacer las pruebas $t$ y $F$ que vimos anteriormente. 

Para revisar si se cumple solo basta hacer una `qqplot` de los residuos. 


**Caso ideal**
```{r}
x <- rnorm(1000)
y <- 1 + x + rnorm(1000, sd = 1)
fit <- lm(y ~ x)
```


```{r}
qqnorm(residuals(fit), asp = 1)
qqline(residuals(fit), col = "red")
```

**Caso errores auto-correlacionados**

```{r}
x <- rnorm(1000)
y <- 1 + x + diffinv(rnorm(999, sd = 1), lag = 1)
fit <- lm(y ~ x)
```


```{r}
qqnorm(residuals(fit), asp = 0)
qqline(residuals(fit), col = "red")
```

**Caso no-lineal**

```{r}
x <- rnorm(1000)
y <- x^2 + rnorm(1000, sd = 0.5)
fit <- lm(y ~ x)
```


```{r}
qqnorm(residuals(fit), asp = 0)
qqline(residuals(fit), col = "red")

```

```{r}
x <- rnorm(1000)
y <- x ^ 2 + rnorm(1000, sd = 0.5)
fit <- lm(y ~ x + I(x ^ 2))
summary(fit)
```


```{r}
qqnorm(residuals(fit), asp = 0)
qqline(residuals(fit), col = "red")

```

#### Multicolinealidad


Hay dos formas de detectar multicolinealidad 

1. Analizar la matriz de correlaciones de las variables (solamente detecta colinealidad entre pares). 

2. Analizar la correlación multiple entre un predictor y el resto.


Defina \(R^{2}_{X_{j}\vert X_{-j}}\) como el \(R^{2}\) de la regresión multiple entre \(X_{j}\) vs el resto de covariables. 

Si \(R^{2}_{X_{j}\vert X_{-j}}\) es cercano a 1 entonces hay alta correlación entre \(X_j\) y el resto. 

Defina el factor de inflación de la varianza como:

\begin{equation*}
 \mathrm{VIF}(\hat{\beta}_{j}) = \frac{1}{1-R^{2}_{X_{j}\vert X_{-j}}}
\end{equation*}


Si \(\mathrm{VIF}\) es alto 

- Quitar las variables 
- Combinar variables




Hay muchos paquetes que tienen implementado la función `vif` (car, rms, entre otros).


**Caso variables colineales**


La variable `wt` está en unidades de 1000lb. La convertimos a Kilogramos. 

```{r}
mtcars_kg <- mtcars %>%
  mutate(wt_kg = wt * 1000 * 0.4535 + rnorm(32))


fit_kg <- lm(mpg ~ disp + wt + wt_kg, data = mtcars_kg)
summary(fit_kg)
```

```{r}
library(car)
options(scipen = 1000)

VIFs <- vif(fit_kg)

VIFs <- as.data.frame(VIFs) %>% 
  rownames_to_column(var = "vars")

ggplot(VIFs, aes(x=vars, y=VIFs, group=1)) + 
  geom_point()+
  geom_line()+
  theme_minimal(base_size = 16)


```





### Otros chequeos importantes 

#### Puntos extremos 

Estos puntos son aquellos que $Y_i$ esta lejos de $\hat{Y}_i$, es decir son puntos en donde los residuos son particularmente muy altos. 


Se puede hacer un gráfico de los residuos vs los valores ajustados como en \@ref(fig:grafico-residuos-lineal) y \@ref(fig:grafico-residuos-no-lineal). 


¿Qué tan grande deben ser los residuos?

**Solución:** Se debe escalar los residuos adecuadamente. 

Se construyen los residuos semi-studendizados 

\begin{equation*}
r_{i}^{s} = \frac{e_{i}}{\sqrt{\mathrm{Var}\left(e_{i}\right)}} 
\end{equation*}

donde $e_i=Y_i-\hat Y_i$. Como \(H=X(X^{\top}X)^{-1}X^{\top}\) es la matriz de proyección entonces sabemos que 

\begin{align*}
\hat{Y}&=  H Y \\
e &= Y - \hat{Y}  
\end{align*}

Entonces tenemos que 

\begin{align*}
\mathrm{Var}\left(e\right) 
&=  \mathrm{Var}\left((I-H)Y\right)\\
&= (I-H)^{2}\mathrm{Var}\left(Y\right)\\
&= (I-H) \sigma^{2} 
\end{align*} 

ya que $I-H$ es idempotente. Por lo tanto 

\begin{equation*}
\mathrm{Var}\left(e_{i}\right) = (1-h_{ii}) \sigma^{2}
\end{equation*}

Para cada observación se estandarizan los residuos de siguiente forma 

\begin{equation*}
r_{i}^{s} = \frac{e_i}{\sqrt{(1-h_{ii}) \sigma^{2}}}
\end{equation*}

**Caso sin valores extremos**
```{r}
x <- rnorm(1000)
y <- 1 + x + rnorm(1000, sd = 0.5)
fit <- lm(y ~ x)

X <- model.matrix(y ~ x)
H <- X %*% solve(t(X) %*% X) %*% t(X)
I <-  diag(1, nrow = 1000)
I_H <- I - H
r_sdnt <-  residuals(fit) / sqrt(diag(I_H) * var(y))
plot(fitted(fit), r_sdnt)

fit
```

**Caso con valores extremos**

```{r}
x <- rnorm(1000)
y <- 1 + x + rnorm(1000, sd = 0.5)
y[1:5] <- runif(5, 30, 40)
fit <- lm(y ~ x)

X <- model.matrix(y ~ x)
H <- X %*% solve(t(X) %*% X) %*% t(X)
I <-  diag(1, nrow = 1000)
I_H <- I - H
r_sdnt <-  residuals(fit) / sqrt(diag(I_H) * var(y))
plot(fitted(fit), r_sdnt)

fit
```

#### Puntos de apalancamiento (leverage)


Un outlier puede ser detectado pero aún así este puede no afectar el modelo como un todo. 

El $r_{i}^s$ puede ser alto por 2 razones: 

1. los residuos $e_i$ son altos (un outlier)
2. el valor $h_{ii}$ es cercano a 1. (Se tiene que $0\leq h_{ii}\leq 1$). 

Los valores donde $h_{ii}\approx 1$ se les denomina de **gran apalancamiento**. 

Como la matriz $H$ es de idempotente y de rango completo:

\begin{equation*}
\sum_{i=1}^{n} h_{ii} = p +1 \text{  (Los predictores más el intercepto)   }
\end{equation*}

**Regla empírica:** Si \(h_{ii}>\frac{p+1}{n}\) entonces decimos que el punto de **gran apalancamiento**.


##### Distancia de Cook. 
La distancia de Cook mide la influencia de las observaciones con respecto al ajuste del modelo lineal con $p$ variables. Esta se define como: 

\[
\displaystyle D_i = \frac{\sum\limits_{j=1}^n (\hat{Y}_j - \hat{Y}_{j(-i)})^2}{(p+1) \sigma^2}
\]

donde $\hat{Y}_{j(-i)}$ significa el ajuste del modelo lineal, removiendo la observación $i$-ésima. 

**Caso base**
```{r}
set.seed(42)
apa_df  = data.frame(x = 1:10,
                     y = 10:1 + rnorm(n = 10))
```


```{r}
modelo <-  lm(y ~ x, data = apa_df)
coef(modelo)
```


```{r}
plot(modelo,
     5,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
```
 

```{r}
plot(
  hatvalues(modelo),
  col = c(rep("black", 10), "red"),
  cex = 2,
  pch = 16
)
abline(h = 2 / 10, col = "blue")
```


```{r}
plot(apa_df,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
abline(a = coef(modelo)[1],
       b = coef(modelo)[2],
       col = "blue")

```

**Bajo apalancamiento, residuos grandes, influencia pequeña**

```{r}
p_1 <-  c(5.4, 11)
apa_df_1  <-  rbind(apa_df, p_1)
modelo_1 <-  lm(y ~ x, data = apa_df_1)
coef(modelo_1)
```


```{r}
plot(modelo_1,
     5,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
```




```{r}
plot(
  hatvalues(modelo_1),
  col = c(rep("black", 10), "red"),
  cex = 2,
  pch = 16
)
abline(h = 2 / 11, col = "blue")
```


```{r}
plot(apa_df_1,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
abline(a = coef(modelo)[1],
       b = coef(modelo)[2],
       col = "blue")
abline(a = coef(modelo_1)[1],
       b = coef(modelo_1)[2],
       col = "red")
```


**Alto apalancamiento, residuo pequeño, influencia pequeña** 


```{r}
p_2 <-  c(18, -5.7)
apa_df_2  <-  rbind(apa_df, p_2)
modelo_2 <-  lm(y ~ x, data = apa_df_2)
coef(modelo_2)
```


```{r}
plot(modelo_2,
     5,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
```


```{r}
plot(hatvalues(modelo_2),
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
abline(h = 2 / 11, col = "blue")
```


```{r}
plot(apa_df_2,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
abline(a = coef(modelo)[1],
       b = coef(modelo)[2],
       col = "blue")
abline(a = coef(modelo_2)[1],
       b = coef(modelo_2)[2],
       col = "red")

```


**Alto apalancamiento, residuo altos, influencia grande** 
```{r}
p_3 <- c(14, 5.1)
apa_df_3  <-  rbind(apa_df, p_3)
modelo_3 <-  lm(y ~ x, data = apa_df_3)
coef(modelo_3)
```


```{r}
plot(modelo_3,
     5,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
```



```{r}
plot(hatvalues(modelo_3),
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
abline(h = 2 / 11, col = "blue")
```
 

```{r}
plot(apa_df_3,
     col = c(rep("black", 10), "red"),
     cex = 2,
     pch = 16)
abline(a = coef(modelo)[1],
       b = coef(modelo)[2],
       col = "blue")
abline(a = coef(modelo_3)[1],
       b = coef(modelo_3)[2],
       col = "red")

```
```{r}
?stats:::plot.lm
```
 

```{r}
plot(modelo_3, which = 1:6)
```


```{r}
plot(modelo, which = 1:6)
```

## Ejercicios 

Del libro [@James2013b] 

- Capítulo 3: 1, 3, 4, 5, 8, 9


