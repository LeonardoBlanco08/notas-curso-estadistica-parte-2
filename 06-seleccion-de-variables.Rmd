
# Métodos de selección de variables y regularización.

## Estimación del error de prueba
Recuerden que el error de prueba es el error promedio que se obtiene al usar un método de aprendizaje estadístico para predecir una observación que no está en el conjunto de calibración o entrenamiento. Si se tuviera un conjunto de prueba designado para medir el error de prueba, este último sería muy fácil de calcular. Cuando no se tiene un conjunto de prueba designado, entonces hay dos formas de estimar el error de prueba:

a. Estimar indirectamente el error de prueba al hacerle un ajuste al error de entrenamiento. Es decir, agregarle artificialmente la variabilidad no observada, con el fin de corregir el exceso de sesgo producto de sobreajuste. 
b. Estimar el error de prueba usando técnicas de muestreo sobre el conjunto de entrenamiento.

Por ahora explicaremos la segunda solución:

### Técnica de conjunto de validación 

Dividir aleatoriamente los datos totales en 2 partes:

- Conjunto de entrenamiento: en donde se ajusta el modelo. 
- Conjunto de validación o prueba: en donde el modelo ajustado se usa para realizar predicciones. 

Se usa una medida de validación en cada uno de los dos conjuntos (\(MSE\) en el caso cuantitativo)

1. Puede que el estimador del error de prueba tenga una varianza muy alta, dependiendo de las observaciones que se incluyeron en la muestra de prueba. 
2. El error de validación tiende a ser mayor que el error de entrenamiento debido a que se usa un número pequeño de observaciones en el conjunto de entrenamiento. (sobreestimación del error de prueba o validación)  


### Validación cruzada "Leave-One-Out" (LOOCV)

Una sola observación \( \left( X_{i}, Y_{i} \right) \) se usa en el conjunto de validación. Observaciones restantes, se usan en el conjunto de entrenamiento. Este proceso se repite para $i=1,\ldots,n$.  

Defina 

\begin{equation*}
MSE_{i} =(Y_{i}-\hat{Y}_{i})^{2}
\end{equation*}
 
como el error cometido por usar la observación \(i\) como muestra de prueba y el resto de valores como muestra de entrenamiento. 

El estimador LOOCV es 

\begin{equation*}
CV_{n} = \frac{1}{n} \sum_{i=1}^{n} MSE_{i}
\end{equation*}


**Ventajas**

1. Menos sesgo. (Conjunto de prueba de tamaño casi igual que los datos totales). Es decir el error de prueba no tiende a sobrestimarse.
2. No hay aleatoriedad en la aproximación del error de prueba producto de la separación conjunto de prueba-entrenamiento.

**Desventaja**: Puede ser lento, dependiendo de la cantidad de datos.

En el caso de un modelo lineal, se puede calcular $CV_n$ es una sola iteración:

\begin{align*}
CV_n=\frac 1 n \sum_{i=1}^n\left(\frac{y_i-\hat y_i}{1-h_{ii}}\right)^2
\end{align*}

### Validación cruzada \(k-\)veces

Se aplica el mismo principio que LOOCV, pero se divide la muestra en \(k\) distintas partes (folds). El estimador del error de prueba es: 

\begin{equation*}
CV_{k} = \frac{1}{k} \sum_{i=1}^{k} MSE_{i}
\end{equation*}

donde en este caso $MSE_i$ es el MSE obtenido al utilizar la $i$-ésima parte de la muestra como conjunto de prueba y las restantes $k-1$ partes como conjunto de entrenamiento. Los valores usuales de $k$ son 5 o 10. Note que LOOCV es un caso especial de esta técnica de validación tomando $k=n$.

**Ventaja**: Es más económico

**Desventaja**: Nivel intermedio de sesgo respecto a las dos técnicas anteriores. 

- Recuerden que entre más grande sea el conjunto de entrenamiento, menos sesgo hay en la estimación de error de prueba.
- El nivel de correlación entre los MSE's de los conjuntos de prueba aumenta si los tamaños de entrenamiento aumentan. Por lo tanto la varianza del error de prueba estimado es mayor en estos casos.

\begin{equation*}
\frac{n}{2} < \frac{(k-1)n}{k} < n-1
\end{equation*}

### Validación cruzada para clasificación

Se usa 

\begin{equation*}
CV_{n} = \frac{1}{n} \sum_{i=1}^{n} Err_{i}
\end{equation*}


donde \(Err_i = I_{Y_i \neq \hat{Y}_{i}}\) y \(CV_{k}\) se define de manera análoga. 

### Otras medidas de error de prueba

#### $R^2$ ajustado

Recuerde que $R^2 = 1 - \dfrac{RSS}{TSS}$. Como $RSS$ decrece si se le agrega más variables, entonces $R^2 \nearrow 1$. Recuerden que $RSS = \sum(y_i-\hat{y}_i)^2$ y $TSS = \sum(y_i-\bar{y}_i)^2$. En este caso definimos:

$$R^2 \text{ ajustado}= 1-\dfrac{\dfrac{RSS}{n-p-1}}{\dfrac{TSS}{n-1}}$$

#### $C_p$ de Mallows

Este estadístico se define como 


$$ C_p = \dfrac{RSS}{\hat\sigma^2} + 2p-n $$


donde $p$ es el número de predictores y $\hat\sigma^2$ es el estimador de la varianza de los errores $\epsilon$. Si $\hat\sigma^2$ es insesgado de $\sigma^2$, entonces $C_p$ es un estimador insesgado del $MSE$ de prueba. 

Para regresión ordinaria sabemos que \(\hat{\boldsymbol{\beta}}_{p}=\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime} \mathbf{Y}\). Nuestro caso ideal sería hacer el MSE lo más pequeño posible entre todos los posibles modleos,
\begin{equation*}
\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}.
\end{equation*}

Con esto en mente, calculemos el RSS del modelo,

\begin{equation*}
\begin{aligned}
\operatorname{RSS}(p) &=\sum_{n=1}^{N}\left(y_{n}-\mathbf{x}_{n} \hat{\boldsymbol{\beta}}_{p}\right)^{2} \\
&=\left(\mathbf{Y}-\mathbf{X}_{p} \hat{\boldsymbol{\beta}}_{p}\right)^{\prime}\left(\mathbf{Y}-\mathbf{X}_{p} \hat{\boldsymbol{\beta}}_{p}\right) \\
&=\mathbf{Y}^{\prime}\left(\mathbf{I}_{N}-\mathbf{X}_{p}\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime}\right) \mathbf{Y}
\end{aligned}
\end{equation*}


Usando este resultado para matrices 

\begin{equation*}
\mathrm{E}\left[\mathbf{Y}^{\prime} \mathbf{A Y}\right]=\mathrm{E}\left[\mathbf{Y}^{\prime}\right] \mathbf{A E}[\mathbf{Y}]+\operatorname{tr}[\mathbf{\Sigma} \mathbf{A}]
\end{equation*}

y donde \(\boldsymbol{\Sigma}\) es la matriz de covarianza de \(\mathbf{Y}\), encontramos que 

\begin{equation*}
\begin{aligned}
\mathrm{E}[\operatorname{RSS}(p)] &=\mathrm{E}\left[\mathbf{Y}^{\prime}\left(\mathbf{I}_{N}-\mathbf{X}_{p}\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime}\right) \mathbf{Y}\right] \\
&=\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}+\operatorname{tr}\left[\mathbf{I}_{N}-\mathbf{X}_{p}\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1} \mathbf{X}_{p}^{\prime}\right] \sigma^{2} \\
&=\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}+\sigma^{2}\left(N-\operatorname{tr}\left[\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)\left(\mathbf{X}_{p}^{\prime} \mathbf{X}_{p}\right)^{-1}\right]\right) \\
&=\mathrm{E}\left[\hat{\boldsymbol{\beta}}_{p}-\boldsymbol{\beta}\right]^{2}+\sigma^{2}(N-p)
\end{aligned}
\end{equation*}

Note que si el  modelo verddero  tiene   \(p\) parametros, entocnes \(\mathrm{E}\left[\mathrm{C}_{p}\right]=p\). Esto muestra por qué, si un modelo  es correcto, \(\mathrm{C}_{p}\) tenderá a estar cerca de \(p\)


Un problema con el criterio \(\mathrm{C}_{p}\) es que tenemos que encontrar una estimación apropiada de \(\sigma^{2}\) para usar con todos los valores de \(p\).

#### Estimador de máxima verosimilitud (MLE)

Supongamos para el conjunto de datos disponible una medida de ajuste del modelo $-\ln L(\hat{\beta} | x)=\ell(\beta)$. Así si el valor es grande, entonces los parámetros \(\beta\)s serían los correctos. Si se define \(\overline{\ell}(\beta)=\mathbb{E}(\ell(\beta\mid X))\) como la log verosimilitud poblacional. 

El problema con está medida es que no penaliza el ingreso de nuevas variables al modelo. 


#### Akaike Information Criterion (AIC).


El AIC se define de la siguiente manera:

\[
AIC = -2\ell(\hat{\beta}) + 2p 
\]

_Explicación breve_: La idea del AIC es ajustar el riesgo empírico \(\hat{R}_n =  -\ell(\hat{\beta})\) con respecto al riesgo verdadero \(R(\hat{\beta}) = \mathbb{E}(-n\overline{\ell}(\hat{\beta}))\). Se puede probar que asintóticamente 

\begin{equation*}
\mathbb{E}\left(\widehat{R}_{n}\left(\hat{\beta}_{n}\right)\right)-R\left(\widehat{\beta}_{n}\right)=-n \mathbb{E}\left(\left(\widehat{\beta}_{n}-\beta^{*}\right)^{T} I\left(\beta^{*}\right)\left(\widehat{\beta}_{n}-\beta^{*}\right)\right)
\end{equation*}



Por el curso de estadísitica I se puede probar que 
\begin{equation*}
\sqrt{n}\left(\hat{\beta}_{n}-\beta^{*}\right) \approx N\left(0, I^{-1}\left(\beta^{*}\right)\right)
\end{equation*}

Por lo tanto, 

\begin{equation*}
n\left(\widehat{\beta}_{n}-\beta^{*}\right)^{T} I\left(\beta^{*}\right)\left(\widehat{\beta}_{n}-\beta^{*}\right) \approx \chi_{d}^{2}
\end{equation*}

lo cual implica que 
\begin{equation*}
n \mathbb{E}\left(\left(\widehat{\beta}_{n}-\beta^{*}\right)^{T} I\left(\beta^{*}\right)\left(\widehat{\beta}_{n}-\beta^{*}\right)\right)=d
\end{equation*}


Entonces para asegurarnos de que el estimador sea insesgado asintóticamente, debemos redefinir nuestro riesgo empírico timador sumándole un término \(p\)


\[\widehat{R}_{n}\left(\widehat{\beta}_{n}\right)+d=-\ell_{n}+d\]



#### Bayesian Information Criterion (BIC)


Este criterior se parece al AIC pero modificado con un \(\log(n)\),

\[
BIC = -2\ell(\hat{\beta}) + \log(n)p. 
\]



Para cada modelo \(m\) Escriba la regla de Bayes definida en el curso anterior 

\begin{equation*}
\pi\left(m \mid X_{1}, \cdots, X_{n}\right)=\frac{\pi\left(m, X_{1}, \cdots, X_{n}\right)}{L\left(X_{1}, \cdots, X_{n}\right)} \propto L\left(X_{1}, \cdots, X_{n} \mid m\right) \pi(m)
\end{equation*}


Se puede demostrar que 

\begin{equation*}
L\left(X_{1}, \cdots, X_{n} \mid \theta, m\right) \approx e^{\ell_{n}-n\left(\theta-\theta^{*}\right)^{T} I\left(\theta^{*}\right)\left(\theta-\theta^{*}\right)}
\end{equation*}


Asumiento que la variable respuesta \(Y\) es gaussiana, se puede simplificar la log-verosimilitud como 


\begin{equation*}
\log p\left(X_{1}, \cdots, X_{n} \mid m\right) \approx \ell_{n}-\frac{d}{2} \log n+\frac{d}{2} \log (2 \pi)+\log \operatorname{det}\left(I\left(\theta^{*}\right)\right)+\log \pi\left(\hat{\theta}_{n} \mid m\right)
\end{equation*}


Las únicas dos cantidades que incrementan con \(n\) son las dos primeras. Se multiplica por -2 para tener  el BIC. 


En el caso de datos gaussiano se tendría lo siguiente:

\begin{align*}
BIC = \frac{1}{n}(RSS+2\log (n)p\hat \sigma^2).
\end{align*}


#### Notas adicionales 

- Una explicación detallada de cada medida la pueden encontrar en el Capítulo 7 [@Hastie2009a] o en el artículo [@CavanaughAkaike2019].
- La validación cruzada LOOCV es asintóticamente equivalente al AIC para modelos de regresión lineal múltiple [@StoneAsymptotic1977]. 
- El AIC ajusta el modo que el riesgo o verosimilitud empírica o real sean insesgadas. Es decir, bajo la observación de nuevos datos, el error que se cometería debería ser cercano a 0. 
- Aunque el BIC se parece al AIC, el razonamiento es algo diferente. En la construcción de BIC estamos seleccionando el modelo con mayor evidencia según los datos. Cuando los datos se generan de hecho a partir de uno de los modelos en la colección de modelos que estamos eligiendo, el posterior se concentrará en este modelo correcto.  
- Una forma de interpretar ambos criterios es que AIC elige el mejor modelo predictivo, mientras que BIC intenta seleccionar el modelo verdadero si existe en el conjunto de modelos.




## Selección de variables

Cuando se construye un modelo de regresión (lineal o logística) existe la posibilidad de que existan más variables que datos disponibles. En este caso definitivamente la matriz de diseño $X$ no sería de rango completo, y otros estadísticos no tendrían una definición clara, por ejemplo el \(R^{2}\) ajustado tenía un factor \(n-p-1\) en el denominador y si \(n>p\) este tipo de indicador no se podría estimar.  

En este capítulo veremos cómo construir modelos más simples y cómo hacer comparaciones entre ellos. 

  
  
  
### Selección del mejor subconjunto.

En este caso trataremos de seleccionar el mejor subconjunto de un total de \(p\) variables. Claramente si escogieramos solo \(k\)  variables existiría un total de $\binom{p}{k}$ modelos diferentes que escoger. Por lo tanto existe un total de $2^p$ posibles modelos para escoger con cualquier número de covariables. 


El algoritmo para este caso sería: 


*Algoritmo:*

1. Sea $M_0$ el modelo nulo.

2.  Para $k=1,2,\dots,p$ (número de variables),
    a. Ajuste todos los $\binom{p}{k}$ modelos que contengan $k$ predictores.
    b. Seleccione el mejor entre esos $\binom{p}{k}$ modelos. El "mejor" es el que tenga el $RSS$ menor, o el $R^2$ más grande. Llame a este modelo $M_k$. 
    
3.  Seleccione el mejor modelo entre $M_0,M_1,\dots,M_p$  usando el error de validación cruzada, $C_p$, $AIC$, $BIC$ o $R^2$ ajustado.

**Nota: Más adelante veremos qué es validación cruzada, \(C_p\),  \(AIC\) y \(BIC\)**

*Ejemplo:* $Y = \beta_0+\beta_1X_1+ \beta_2X_2 + \beta_3X_3$. 

Puede ser que el mejor modelo sea

* $Y = \beta_0$, 

* $Y = \beta_0+\beta_1X_1$, 

* $Y = \beta_0+\beta_2X_2$, 

* $Y = \beta_0+\beta_3X_3$,

* $Y = \beta_0+\beta_1X_1+\beta_2X_2$,

* $Y = \beta_0+\beta_1X_1+\beta_3X_3$, entre otras.

De los que tienen $k=1$ variable, hay $\binom{3}{1}$ = 3 modelos. Para $k=2$, son $\binom{3}{2}$ = 3, y para $k=3$, solo un modelo. Para $k=1$, se ajustan los 3 y al mejor se le llama $M_1$. Así para los otros $k$. Obtenidos estos modelos, se escoge el que tenga la mejor medida, con respecto a los errores antes mencionados.

*Notas:* 

- La parte 2.b. se hace con la muestra de entrenamiento. **Objetivo: Minimizar el error de entrenamiento.**
* La parte 3 se selecciona con los datos de prueba. **Objetivo: Minimizar el error de prueba**. 
- Si se usa el $RSS$ o $R^2$, siempre se selecciona el modelo con el número mayor de variables. Esto puede provocar sobreajuste.
- El principal inconveniente de este método es que es computacionalmente ineficiente cuando $p$ es relativamente grande.




### Selección de modelos hacia adelante (**Forward Stepwise Selection**)

*Algoritmo:*

1. Sea $M_0$ el modelo nulo. 
1. Para $k=0,1,\dots,p-1$,
    a. Considere los $p-k$ modelos que contenga los predictores en $M_k$ con un predictor adicional.
    a. Seleccione el mejor entre esos $p-k$ modelos usando el $R^2$ o $RSS$. Llámelo $M_{k+1}$.
1. Seleccione el mejor modelo entre $M_0,\dots, M_p$ usando $CV$, $C_p$, $AIC$, $BIC$ o $R^2$ ajustado.


*Ejemplo:*  $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3$

* $M_0$: $Y = \beta_0$

* $M_1$: $Y = \beta_0+\beta_1X_1$, $Y = \beta_0+\beta_2X_2$ o $Y = \beta_0+\beta_3X_3$. De los tres se escoge el mejor (por ejemplo, el segundo) y se le llama $M_1$. 

* $M_2$: a $Y = \beta_0+\beta_2X_2$, que es $M_1$, se le suma una variable extra ($\beta_1X_1$ o $\beta_3X_3$) y se selecciona el mejor. 

* $M_3$: $M_2$ más la variable no incluida.

*Nota:* 

- El número de modelos por calcular usando el mejor subconjunto  es $2^p$, mientras que usando Forward Selection es $1+\displaystyle\sum_0^ {p-1} p-k = \dfrac{1+p(1+p)}2$.

- No hay garantía de que el modelo seleccionado con este método sea el mejor de todos los posibles.

### Selección de modelos hacia atrás (**Backward Stepwise Selection**)

*Algoritmo:*

1. Sea $M_p$ el modelo completo.
1. Para $k=p,p-1,\dots,1$,
    a. Considere los $k$ modelos que contienen todos excepto uno de los predictores en $M_k$ para un total de $k-1$ predictores.
    a. Seleccione el mejor entre esos $k$ modelos usando el $R^2$ o $RSS$. Llámelo $M_{k+1}$.
1. Seleccione el mejor modelo entre $M_0,\dots,M_p$ usando $CV$, $C_p$, $AIC$, $BIC$ o $R^2$ ajustado.


*Ejemplo:*  $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3$

* $M_3$: $Y = \beta_0 +\beta_1X_1+\beta_2X_2+\beta_3X_3$.

* $M_2$: se quita una variable ($X_1$, $X_2$ o $X_3$) y se selecciona el mejor. Por ejemplo, se remueve $X_1$.

* $M_1$: A $M_{2}$ le quito otra variable. En este caso, $X_2$ o $X_3$ y se escoge el mejor.

* $M_0$: $Y=\beta_0$, el modelo nulo.

Las dos notas del método de Forward Selection aplican para este caso también.

## Métodos de regularización

Una forma alternativa de hacer selección de variables es a través de la restricción o regularización de los coeficientes de los modelos. 

### Regresión Ridge

Considere el problema de mínimos cuadrados en el modelo lineal:

$$ RSS = \sum_{i=1}^{n}\left(y_i-\sum_{j=1}^{p}\beta_jX_{ij}\right)^2 $$
y 
\[
\hat\beta = \underset{\beta}{\mathrm{argmin}} RSS
\]


La regresión Ridge consiste en determinar

$$ \hat\beta^R_\lambda = \underset{\beta}{\mathrm{argmin}}\left[RSS + \lambda\|\beta_{-0}\|^2_2 \right]$$

donde:
$$\|\beta_{-0}\|^2_2 = \sum_{j=1}^{p}\beta_j^2$$
¿Cómo se debe seleccionar el $\lambda$?
El método para seleccionarlo es por validación cruzada

*Nota:*

- Si $\lambda = 0$, $\hat\beta = \beta^R_\lambda$: caso de máxima varianza, con el menor sesgo posible.
- Si $\lambda \to +\infty$, $\beta \to 0$: se sacrifican todos los parámetros $\beta$. Máximo sesgo pero varianza nula.
- Cuando $p$ es cercano a $n$, la varianza a nivel de parámetros es alta (matriz $X$ deja de comportarse como de rango completo). En ese caso la regresión ridge se convierte en una solución para disminuir esa varianza.
- Si $p>n$ (mayor cantidad de variables que observaciones), al realizar mínimos cuadrados, no existe una solución única, pero con la forma de regresión de Ridge es posible alcanzarla para un cierto valor de $\lambda$.
- La regresión ridge tiene ventajas computacionales comparado con el método de selección de todos los subconjuntos.
- Los estimadores de ridge no son invariantes a cambios de escala (transformaciones $cX_i$), a diferencia de los estimadores por mínimos cuadrados. Por lo tanto se recomienda estandarizar todas los predictores antes de aplicar ridge.



### Regresión Lasso

La regresión ridge tiene el inconveniente de que incluye todas los predictores en el modelo, lo cual puede ser un inconveniente para efectos interpretativos (de parámetros) pero no tanto para efectos predictivos, especialmente cuando el número de predictores es alto. Para solucionar este problema se plantea:

$$ \beta_{\lambda}^{LASSO} = \underset{\beta}{\mathrm{argmin}}\left(RSS + \lambda\|\beta_{-0}\|_1 \right)$$
donde:
$$ \|\beta_{-0}\|_1 = \sum_{j=1}^p|\beta_j|$$
Otra formulación para los métodos vistos son:


1. \textbf{Ridge}: $\underset{\beta}{\min} RSS$, sujeto a $\displaystyle\sum_{j=1}^p\beta_j^2 \leq s$.
1. \textbf{Lasso}: $\underset{\beta}{\min} RSS$, sujeto a $\displaystyle\sum_{j=1}^p|\beta_j| \leq s$.
<!-- 1. \textbf{Mejor subconjunto}: $\underset{\beta}{\min} RSS$, sujeto a $\displaystyle\sum_{j=1}^p I_{\lbrace\beta_j \neq 0\rbrace} \leq s$. -->

*Nota:* la regresión lasso causa una selección de predictores debido a que $RSS$ suele intersecar la región de penalización en esquinas.

### Explicación gráfica

![Tomado de [DataSklr](https://www.datasklr.com/extensions-of-ols-regression/regularization-and-shrinkage-ridge-lasso-and-elastic-net-regression)](manual_figures/ridge-lasso.png)



![Tomado de [Towards to Data Science](https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd)](manual_figures/ridge-lasso2.png)

## Laboratorio

### Cross-Validation

#### Leave-one-out Cross Validation (LOOCV)

Es posible comparar distintos ajustes de modelos usando cross-validation.

Carguemos la base de datos `Auto` de `ISLR`.


```{r}
library(ISLR)
library(GGally)
data("Auto")
```

```{r}
ggpairs(Auto[,-9])
```

```{r}
summary(Auto)
```

Y ajustamos un modelo entre las millas por galon contra los caballos de fuerza de ciertos vehículos.

```{r}
glm.fit <- glm(mpg ~ horsepower, data = Auto)
summary(glm.fit)
```

La librería `boot` tiene funciones para aplicar cross-validation. En particular el valor `delta` dice lo siguiente 

```
delta: A vector of length two.  The first component is the raw
       cross-validation estimate of prediction error.  The second
       component is the adjusted cross-validation estimate.  The
       adjustment is designed to compensate for the bias introduced
       by not using leave-one-out cross-validation.
```


Este valor se puede visualizar de la siguiente manera:
  
```{r}
library(boot)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```

En particular se puede usar un `for` para aplicar este mismo procedimiento a múltiples modelos. En el siguiente caso trataremos de ajustar los modelos \(\beta_1 x\), \(\beta_1 x + \beta_2 x^2\), \(\beta_1 x + \beta_2 x^2 +\beta_3 x^{3}\), etc

```{r}
cv.error.LOOCV = rep(0, 5)
for (i in 1:5) {
  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.LOOCV[i] = cv.glm(Auto, glm.fit)$delta[1]
}
cv.error.LOOCV
plot(cv.error.LOOCV, type = "l")
```

#### K-Fold Cross Validation

Este procedimiento se puede repetir con los el K-fold.

```{r}
set.seed(17)
cv.error.10 = rep(0, 10)
for (i in 1:10) {
  glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10

plot(cv.error.LOOCV, type = "l", col = "red")
lines(cv.error.10, type = "l", col = "blue")
```
### Selección de variables

Cargue los datos `Hitters` del paquete `ISLR` que representan el salario de varios jugadores de beisbol y sus estadística de juego (número de bateos, home runs, carreras, etc.).

#### Análisis exploratorio

Con esta información, haga un análisis exploratorio de los datos usando `ggpairs`.

```{r}
library(GGally)
ggpairs(Hitters)
summary(Hitters)
```

Para limpiar la base de datos de `NA` usamos `dplyr`.

```{r}
library(tidyverse)
Hitters <- Hitters %>% drop_na()
```

Cargue la librería

```{r}
library(leaps)
```

y busque la ayuda de la función `regsubsets`. Use esta función para ajustar todo los posibles modelos de la forma `Salary ~ .`.

Puede guardar estos modelos en ciertas variables (e.g. `regfit.full`) y usar la función `plot`.

```{r}
library(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters, nvmax = 19)
regfit.full.summary <- summary(regfit.full)
regfit.full.summary
```

```{r}
str(regfit.full.summary)
```

```{r}
idx <- which.max(regfit.full.summary$adjr2)
plot(regfit.full.summary$adjr2)
points(idx, regfit.full.summary$adjr2[idx], col = "red", 
       cex = 2,pch = 20)
```


```{r}
idx <- which.min(regfit.full.summary$cp)
plot(regfit.full.summary$cp)
points(idx, regfit.full.summary$cp[idx], col = "red", 
       cex = 2,pch = 20)
```


```{r}
idx <- which.min(regfit.full.summary$bic)
plot(regfit.full.summary$bic)
points(idx, regfit.full.summary$bic[idx], col = "red", 
       cex = 2,pch = 20)
```

```{r}
plot(regfit.full, scale = "bic")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "adjr2")
coef(regfit.full, 10)
```

#### Regresión forward y backward

La función `regsubsets` tiene un paramétro `method`. Usen los valores `forward` y `backward` y comparen los resultados.

Puede guardar estos modelos en ciertas variables (e.g. `regfit.fwd` y `regfit.bwd`) y usar la función `plot`.

```{r}
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, 
                         method = "forward")
summary(regfit.fwd)
```


```{r}
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, 
                         method = "backward")
summary(regfit.bwd)
```


```{r}
par(mfrow = c(1, 2))
plot(regfit.fwd, scale = "bic")
plot(regfit.bwd, scale = "bic")
```


```{r}
par(mfrow = c(1, 2))
plot(regfit.fwd, scale = "Cp")
plot(regfit.bwd, scale = "Cp")
```


```{r}
par(mfrow = c(1, 2))
plot(regfit.fwd, scale = "adjr2")
plot(regfit.bwd, scale = "adjr2")
```

#### Regresión Ridge

```{r}
mm <- model.matrix(Salary~.,Hitters)[,-1]
x <- mm[,-1]
y <- mm[,1]
```

Usando el paquete `glmnet` y la función con el mismo nombre, ejecute el siguiente comando

```{r}
library(glmnet)
grid <- 10 ^ seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```
El factor `lambda` representa el $\lambda$ de la fórmula

$$ \hat{\beta} = \underset{\beta}{\mathrm{argmin}} \left\{RSS + \lambda \Vert \beta \Vert_2^2\right\}.$$

Si no se incluye el paramétro lambda del modelo, R construye una secuencia de $\lambda'$s estimados por validación cruzada.

Haga lo siguiente:

1. Construya un modelo usando todos los datos (sin separar muestra de entrenamiento y prueba).

2. Construya el siguiente modelo


```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- -train
y.test <- y[test]
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])

# MSE
mean((ridge.pred - y.test)^2)
```

¿Qué ocurre si se cambia el paramétro $s$ de `predict` por un `10e10` (i.e. $10^{10}$). Comente los resultados. ¿Y qué ocurre si $s=0$?

3. Finalmente, ejecute el siguiente código

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
```

Busque la ayuda de `cv.glmnet` y deduzca qué significa el gráfico.

```{r}
library(glmnet)
grid <- 10 ^ seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```


```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
```


```{r}
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
```


```{r}
sqrt(sum(coef(ridge.mod)[-1,60]^2))
plot(ridge.mod)
ridge.mod <- glmnet(x, y, alpha = 0)
```


```{r}
ridge.mod$lambda
plot(ridge.mod)
predict(ridge.mod, s = 50, type = "coefficients")
```


```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- -train
y.test <- y[test]
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.pred <- predict.glmnet(ridge.mod, s = 4, newx = x[test, ], exact = FALSE)
```


```{r}
# MSE
mean((ridge.pred - y.test)^2)

ridge.pred <- predict(ridge.mod, s = 1e+10, newx = x[test, ], exact = TRUE)
mean((ridge.pred - y.test)^2)

ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ], exact = FALSE)
mean((ridge.pred - y.test)^2)

lm(y ~ x, subset = train)

predict(ridge.mod, s = 0, type = "coefficients", exact = FALSE)

set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)

plot(cv.out)

bestlam <- cv.out$lambda.min
bestlam

log(bestlam)

ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
mean((ridge.pred - y.test)^2)

out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam*1.10, exact = FALSE)
```

### Regresión Lasso

Ejecute los procedimientos anteriores con glmnet pero modifique el paramétro `alpha = 0`. Compare los resultados.

En este caso, se están encontrando los valores de $\beta$ tal que
$$\hat{\beta} = \underset{\beta}{\mathrm{argmin}} \left\{RSS + \lambda \Vert \beta \Vert_1^2\right\}.$$
```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```


```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
```


```{r}
bestlam <- cv.out$lambda.min
bestlam
```


```{r}
log(bestlam)
```


```{r}
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test) ^ 2)
```


```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <-
  predict(out, type = "coefficients", s = bestlam*1.10)
lasso.coef
```


```{r}
lasso.coef[lasso.coef != 0]
```


## Ejercicios 

- Del libro [@James2013b] 
    - Capítulo 5. 2, 5, 8.
    - Capítulo 6: 5, 6, 7, 8, 10.


